// Targeted by JavaCPP version 1.5.11: DO NOT EDIT THIS FILE

package us.ihmc.llamacpp;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static us.ihmc.llamacpp.global.llamacpp.*;


    @Properties(inherit = us.ihmc.llamacpp.LlamaCPPConfig.class)
public class llama_model_params extends Pointer {
        static { Loader.load(); }
        /** Default native constructor. */
        public llama_model_params() { super((Pointer)null); allocate(); }
        /** Native array allocator. Access with {@link Pointer#position(long)}. */
        public llama_model_params(long size) { super((Pointer)null); allocateArray(size); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public llama_model_params(Pointer p) { super(p); }
        private native void allocate();
        private native void allocateArray(long size);
        @Override public llama_model_params position(long position) {
            return (llama_model_params)super.position(position);
        }
        @Override public llama_model_params getPointer(long i) {
            return new llama_model_params((Pointer)this).offsetAddress(i);
        }
    
        // NULL-terminated list of devices to use for offloading (if NULL, all available devices are used)

        public native int n_gpu_layers(); public native llama_model_params n_gpu_layers(int setter); // number of layers to store in VRAM
        public native llama_split_mode split_mode(); public native llama_model_params split_mode(llama_split_mode setter); // how to split the model across multiple GPUs

        // the GPU that is used for the entire model when split_mode is LLAMA_SPLIT_MODE_NONE
        public native int main_gpu(); public native llama_model_params main_gpu(int setter);

        // proportion of the model (layers or rows) to offload to each GPU, size: llama_max_devices()
        public native @Const FloatPointer tensor_split(); public native llama_model_params tensor_split(FloatPointer setter);

        // Called with a progress value between 0.0 and 1.0. Pass NULL to disable.
        // If the provided progress_callback returns true, model loading continues.
        // If it returns false, model loading is immediately aborted.
        public native llama_progress_callback progress_callback(); public native llama_model_params progress_callback(llama_progress_callback setter);

        // context pointer passed to the progress callback
        public native Pointer progress_callback_user_data(); public native llama_model_params progress_callback_user_data(Pointer setter);

        // override key-value pairs of the model meta data
        public native @Const llama_model_kv_override kv_overrides(); public native llama_model_params kv_overrides(llama_model_kv_override setter);

        // Keep the booleans together to avoid misalignment during copy-by-value.
        public native @Cast("bool") boolean vocab_only(); public native llama_model_params vocab_only(boolean setter);    // only load the vocabulary, no weights
        public native @Cast("bool") boolean use_mmap(); public native llama_model_params use_mmap(boolean setter);      // use mmap if possible
        public native @Cast("bool") boolean use_mlock(); public native llama_model_params use_mlock(boolean setter);     // force system to keep model in RAM
        public native @Cast("bool") boolean check_tensors(); public native llama_model_params check_tensors(boolean setter); // validate model tensor data
    }
