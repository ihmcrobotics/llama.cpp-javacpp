// Targeted by JavaCPP version 1.5.11: DO NOT EDIT THIS FILE

package us.ihmc.llamacpp;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static us.ihmc.llamacpp.global.llamacpp.*;


    // NOTE: changing the default values of parameters marked as [EXPERIMENTAL] may cause crashes or incorrect results in certain configurations
    //       https://github.com/ggml-org/llama.cpp/pull/7544
    @Properties(inherit = us.ihmc.llamacpp.LlamaCPPConfig.class)
public class llama_context_params extends Pointer {
        static { Loader.load(); }
        /** Default native constructor. */
        public llama_context_params() { super((Pointer)null); allocate(); }
        /** Native array allocator. Access with {@link Pointer#position(long)}. */
        public llama_context_params(long size) { super((Pointer)null); allocateArray(size); }
        /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
        public llama_context_params(Pointer p) { super(p); }
        private native void allocate();
        private native void allocateArray(long size);
        @Override public llama_context_params position(long position) {
            return (llama_context_params)super.position(position);
        }
        @Override public llama_context_params getPointer(long i) {
            return new llama_context_params((Pointer)this).offsetAddress(i);
        }
    
        public native @Cast("uint32_t") int n_ctx(); public native llama_context_params n_ctx(int setter);             // text context, 0 = from model
        public native @Cast("uint32_t") int n_batch(); public native llama_context_params n_batch(int setter);           // logical maximum batch size that can be submitted to llama_decode
        public native @Cast("uint32_t") int n_ubatch(); public native llama_context_params n_ubatch(int setter);          // physical maximum batch size
        public native @Cast("uint32_t") int n_seq_max(); public native llama_context_params n_seq_max(int setter);         // max number of sequences (i.e. distinct states for recurrent models)
        public native int n_threads(); public native llama_context_params n_threads(int setter);         // number of threads to use for generation
        public native int n_threads_batch(); public native llama_context_params n_threads_batch(int setter);   // number of threads to use for batch processing

        public native @Cast("llama_rope_scaling_type") int rope_scaling_type(); public native llama_context_params rope_scaling_type(int setter); // RoPE scaling type, from `enum llama_rope_scaling_type`      // whether to pool (sum) embedding results by sequence id
        public native @Cast("llama_attention_type") int attention_type(); public native llama_context_params attention_type(int setter);    // attention type to use for embeddings

        // ref: https://github.com/ggml-org/llama.cpp/pull/2054
        public native float rope_freq_base(); public native llama_context_params rope_freq_base(float setter);   // RoPE base frequency, 0 = from model
        public native float rope_freq_scale(); public native llama_context_params rope_freq_scale(float setter);  // RoPE frequency scaling factor, 0 = from model
        public native float yarn_ext_factor(); public native llama_context_params yarn_ext_factor(float setter);  // YaRN extrapolation mix factor, negative = from model
        public native float yarn_attn_factor(); public native llama_context_params yarn_attn_factor(float setter); // YaRN magnitude scaling factor
        public native float yarn_beta_fast(); public native llama_context_params yarn_beta_fast(float setter);   // YaRN low correction dim
        public native float yarn_beta_slow(); public native llama_context_params yarn_beta_slow(float setter);   // YaRN high correction dim
        public native @Cast("uint32_t") int yarn_orig_ctx(); public native llama_context_params yarn_orig_ctx(int setter);    // YaRN original context size
        public native float defrag_thold(); public native llama_context_params defrag_thold(float setter);     // defragment the KV cache if holes/size > thold, < 0 disabled (default)
        public native Pointer cb_eval_user_data(); public native llama_context_params cb_eval_user_data(Pointer setter); // data type for K cache [EXPERIMENTAL] // data type for V cache [EXPERIMENTAL]

        // Keep the booleans together and at the end of the struct to avoid misalignment during copy-by-value.
        // TODO: move at the end of the struct
        public native @Cast("bool") boolean logits_all(); public native llama_context_params logits_all(boolean setter);  // the llama_decode() call computes all logits, not just the last one (DEPRECATED - set llama_batch.logits instead)
        public native @Cast("bool") boolean embeddings(); public native llama_context_params embeddings(boolean setter);  // if true, extract embeddings (together with logits)
        public native @Cast("bool") boolean offload_kqv(); public native llama_context_params offload_kqv(boolean setter); // whether to offload the KQV ops (including the KV cache) to GPU
        public native @Cast("bool") boolean flash_attn(); public native llama_context_params flash_attn(boolean setter);  // whether to use flash attention [EXPERIMENTAL]
        public native @Cast("bool") boolean no_perf(); public native llama_context_params no_perf(boolean setter);     // whether to measure performance timings

        // Abort callback
        // if it returns true, execution of llama_decode() will be aborted
        // currently works only with CPU execution
        public native Pointer abort_callback_data(); public native llama_context_params abort_callback_data(Pointer setter);
    }
