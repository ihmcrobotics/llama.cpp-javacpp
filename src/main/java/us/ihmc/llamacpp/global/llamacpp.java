// Targeted by JavaCPP version 1.5.11: DO NOT EDIT THIS FILE

package us.ihmc.llamacpp.global;

import us.ihmc.llamacpp.*;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

public class llamacpp extends us.ihmc.llamacpp.LlamaCPPConfig {
    static { Loader.load(); }

// Parsed from ggml-backend.h

// #pragma once

// #include "ggml.h"
// #include "ggml-alloc.h"

// #ifdef GGML_BACKEND_SHARED
// #    if defined(_WIN32) && !defined(__MINGW32__)
// #        ifdef GGML_BACKEND_BUILD
// #            define GGML_BACKEND_API __declspec(dllexport) extern
// #        else
// #            define GGML_BACKEND_API __declspec(dllimport) extern
// #        endif
// #    else
// #        define GGML_BACKEND_API __attribute__ ((visibility ("default"))) extern
// #    endif
// #else
// #    define GGML_BACKEND_API extern
// #endif

// #ifdef  __cplusplus
// Targeting ../ggml_backend_buffer_type.java


// Targeting ../ggml_backend_buffer.java


// Targeting ../ggml_backend_event.java


// Targeting ../ggml_backend.java


// Targeting ../ggml_backend_graph_plan_t.java


// Targeting ../ggml_backend_reg.java


// Targeting ../ggml_backend_device.java




    //
    // Backend buffer type
    //

    //
    // Backend buffer
    //

    public enum ggml_backend_buffer_usage {
        GGML_BACKEND_BUFFER_USAGE_ANY(0),
        GGML_BACKEND_BUFFER_USAGE_WEIGHTS(1),
        GGML_BACKEND_BUFFER_USAGE_COMPUTE(2);

        public final int value;
        private ggml_backend_buffer_usage(int v) { this.value = v; }
        private ggml_backend_buffer_usage(ggml_backend_buffer_usage e) { this.value = e.value; }
        public ggml_backend_buffer_usage intern() { for (ggml_backend_buffer_usage e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }

    public static native @Cast("const char*") BytePointer ggml_backend_buffer_name(ggml_backend_buffer buffer);
    public static native void ggml_backend_buffer_free(ggml_backend_buffer buffer);
    public static native Pointer ggml_backend_buffer_get_base(ggml_backend_buffer buffer);
    public static native @Cast("size_t") long ggml_backend_buffer_get_size(ggml_backend_buffer buffer);
    public static native @ByVal ggml_status ggml_backend_buffer_init_tensor(ggml_backend_buffer buffer, ggml_tensor tensor);
    public static native @Cast("size_t") long ggml_backend_buffer_get_alignment(ggml_backend_buffer buffer);
    public static native @Cast("size_t") long ggml_backend_buffer_get_max_size(ggml_backend_buffer buffer);
    public static native @Cast("size_t") long ggml_backend_buffer_get_alloc_size(ggml_backend_buffer buffer, ggml_tensor tensor);
    public static native void ggml_backend_buffer_clear(ggml_backend_buffer buffer, @Cast("uint8_t") byte value);
    public static native @Cast("bool") boolean ggml_backend_buffer_is_host(ggml_backend_buffer buffer);
    public static native void ggml_backend_buffer_set_usage(ggml_backend_buffer buffer, ggml_backend_buffer_usage usage);
    public static native void ggml_backend_buffer_set_usage(ggml_backend_buffer buffer, @Cast("ggml_backend_buffer_usage") int usage);
    public static native ggml_backend_buffer_usage ggml_backend_buffer_get_usage(ggml_backend_buffer buffer);
    public static native void ggml_backend_buffer_reset(ggml_backend_buffer buffer);

    // tensor copy between different backends
    public static native void ggml_backend_tensor_copy(ggml_tensor src, ggml_tensor dst);

    //
    // Backend (stream)
    //
    public static native @Cast("const char*") BytePointer ggml_backend_name(ggml_backend backend);
    public static native void ggml_backend_free(ggml_backend backend);
    public static native ggml_backend_buffer ggml_backend_alloc_buffer(ggml_backend backend, @Cast("size_t") long size);
    public static native @Cast("size_t") long ggml_backend_get_alignment(ggml_backend backend);
    public static native @Cast("size_t") long ggml_backend_get_max_size(ggml_backend backend);

    public static native void ggml_backend_tensor_set_async(ggml_backend backend,       ggml_tensor tensor, @Const Pointer data, @Cast("size_t") long offset, @Cast("size_t") long size);
    public static native void ggml_backend_tensor_get_async(ggml_backend backend, @Const ggml_tensor tensor,       Pointer data, @Cast("size_t") long offset, @Cast("size_t") long size);

    // "offset" refers to the offset in tensor->data for setting/getting data
    public static native void ggml_backend_tensor_set(      ggml_tensor tensor, @Const Pointer data, @Cast("size_t") long offset, @Cast("size_t") long size);
    public static native void ggml_backend_tensor_get(@Const ggml_tensor tensor,       Pointer data, @Cast("size_t") long offset, @Cast("size_t") long size);
    public static native void ggml_backend_tensor_memset(   ggml_tensor tensor,     @Cast("uint8_t") byte value, @Cast("size_t") long offset, @Cast("size_t") long size);

    public static native void ggml_backend_synchronize(ggml_backend backend);

    public static native ggml_backend_graph_plan_t ggml_backend_graph_plan_create(ggml_backend backend, ggml_cgraph cgraph);
    public static native void ggml_backend_graph_plan_free(ggml_backend backend, ggml_backend_graph_plan_t plan);

    public static native @ByVal ggml_status ggml_backend_graph_plan_compute(ggml_backend backend, ggml_backend_graph_plan_t plan);
    public static native @ByVal ggml_status ggml_backend_graph_compute(ggml_backend backend, ggml_cgraph cgraph);
    public static native @ByVal ggml_status ggml_backend_graph_compute_async(ggml_backend backend, ggml_cgraph cgraph);

    // NOTE: will be removed, use device version instead
    public static native @Cast("bool") boolean ggml_backend_supports_op(ggml_backend backend, @Const ggml_tensor op);
    public static native @Cast("bool") boolean ggml_backend_offload_op(ggml_backend backend, @Const ggml_tensor op);

    // asynchronous copy
    // the copy is performed after all the currently queued operations in backend_src
    // backend_dst will wait for the copy to complete before performing other operations
    // automatic fallback to sync copy if async is not supported
    public static native void ggml_backend_tensor_copy_async(ggml_backend backend_src, ggml_backend backend_dst, ggml_tensor src, ggml_tensor dst);

    public static native ggml_backend_device ggml_backend_get_device(ggml_backend backend);

    //
    // Events
    //

    public static native ggml_backend_event ggml_backend_event_new(ggml_backend_device device);
    public static native void ggml_backend_event_free(ggml_backend_event event);
    public static native void ggml_backend_event_record(ggml_backend_event event, ggml_backend backend);
    public static native void ggml_backend_event_synchronize(ggml_backend_event event);
    public static native void ggml_backend_event_wait(ggml_backend backend, ggml_backend_event event);

    //
    // Backend device
    //

    
// Targeting ../ggml_backend_dev_caps.java


// Targeting ../ggml_backend_dev_props.java



    public static native @Cast("const char*") BytePointer ggml_backend_dev_name(ggml_backend_device device);
    public static native @Cast("const char*") BytePointer ggml_backend_dev_description(ggml_backend_device device);
    public static native void ggml_backend_dev_memory(ggml_backend_device device, @Cast("size_t*") SizeTPointer _free, @Cast("size_t*") SizeTPointer total);
    
    public static native void ggml_backend_dev_get_props(ggml_backend_device device, ggml_backend_dev_props props);
    public static native ggml_backend_reg ggml_backend_dev_backend_reg(ggml_backend_device device);
    public static native ggml_backend ggml_backend_dev_init(ggml_backend_device device, @Cast("const char*") BytePointer params);
    public static native ggml_backend ggml_backend_dev_init(ggml_backend_device device, String params);
    public static native ggml_backend_buffer ggml_backend_dev_buffer_from_host_ptr(ggml_backend_device device, Pointer ptr, @Cast("size_t") long size, @Cast("size_t") long max_tensor_size);

    public static native @Cast("bool") boolean ggml_backend_dev_supports_op(ggml_backend_device device, @Const ggml_tensor op);
    public static native @Cast("bool") boolean ggml_backend_dev_offload_op(ggml_backend_device device, @Const ggml_tensor op);

    //
    // Backend (reg)
    //

    public static native @Cast("const char*") BytePointer ggml_backend_reg_name(ggml_backend_reg reg);
    public static native @Cast("size_t") long ggml_backend_reg_dev_count(ggml_backend_reg reg);
    public static native ggml_backend_device ggml_backend_reg_dev_get(ggml_backend_reg reg, @Cast("size_t") long index);
    public static native Pointer ggml_backend_reg_get_proc_address(ggml_backend_reg reg, @Cast("const char*") BytePointer name);
    public static native Pointer ggml_backend_reg_get_proc_address(ggml_backend_reg reg, String name);

    // Common functions that may be obtained using ggml_backend_reg_get_proc_address

    // Split buffer type for tensor parallelism
// Targeting ../ggml_backend_set_n_threads_t.java


    // Get additional buffer types provided by the device (returns a NULL-terminated array)
    // Set the abort callback for the backend
// Targeting ../ggml_backend_feature.java


// Targeting ../ggml_backend_get_features_t.java



    //
    // Backend registry
    //

    public static native void ggml_backend_device_register(ggml_backend_device device);

    // Backend (reg) enumeration
    public static native @Cast("size_t") long ggml_backend_reg_count();
    public static native ggml_backend_reg ggml_backend_reg_get(@Cast("size_t") long index);
    public static native ggml_backend_reg ggml_backend_reg_by_name(@Cast("const char*") BytePointer name);
    public static native ggml_backend_reg ggml_backend_reg_by_name(String name);

    // Device enumeration
    public static native @Cast("size_t") long ggml_backend_dev_count();
    public static native ggml_backend_device ggml_backend_dev_get(@Cast("size_t") long index);
    public static native ggml_backend_device ggml_backend_dev_by_name(@Cast("const char*") BytePointer name);
    public static native ggml_backend_device ggml_backend_dev_by_name(String name);

    // Direct backend (stream) initialization
    // = ggml_backend_dev_init(ggml_backend_dev_by_name(name), params)
    public static native ggml_backend ggml_backend_init_by_name(@Cast("const char*") BytePointer name, @Cast("const char*") BytePointer params);
    public static native ggml_backend ggml_backend_init_by_name(String name, String params);
    // = ggml_backend_dev_init(ggml_backend_dev_by_type(type), params)
    // = ggml_backend_dev_init(ggml_backend_dev_by_type(GPU) OR ggml_backend_dev_by_type(CPU), NULL)
    public static native ggml_backend ggml_backend_init_best();

    // Load a backend from a dynamic library and register it
    public static native ggml_backend_reg ggml_backend_load(@Cast("const char*") BytePointer path);
    public static native ggml_backend_reg ggml_backend_load(String path);
    // Unload a backend if loaded dynamically and unregister it
    public static native void ggml_backend_unload(ggml_backend_reg reg);
    // Load all known backends from dynamic libraries
    public static native void ggml_backend_load_all();
    public static native void ggml_backend_load_all_from_path(@Cast("const char*") BytePointer dir_path);
    public static native void ggml_backend_load_all_from_path(String dir_path);
// Targeting ../ggml_backend_sched.java


// Targeting ../ggml_backend_sched_eval_callback.java



    // Initialize a backend scheduler, backends with low index are given priority over backends with high index
    public static native void ggml_backend_sched_free(ggml_backend_sched sched);

    // Initialize backend buffers from a measure graph
    public static native @Cast("bool") boolean ggml_backend_sched_reserve(ggml_backend_sched sched, ggml_cgraph measure_graph); // returns success

    public static native int ggml_backend_sched_get_n_backends(ggml_backend_sched sched);
    public static native ggml_backend ggml_backend_sched_get_backend(ggml_backend_sched sched, int i);

    // Get the number of splits of the last graph
    public static native int ggml_backend_sched_get_n_splits(ggml_backend_sched sched);
    public static native int ggml_backend_sched_get_n_copies(ggml_backend_sched sched);

    public static native @Cast("size_t") long ggml_backend_sched_get_buffer_size(ggml_backend_sched sched, ggml_backend backend);

    public static native void ggml_backend_sched_set_tensor_backend(ggml_backend_sched sched, ggml_tensor node, ggml_backend backend);
    public static native ggml_backend ggml_backend_sched_get_tensor_backend(ggml_backend_sched sched, ggml_tensor node);

    // Allocate and compute graph on the backend scheduler
    public static native @Cast("bool") boolean ggml_backend_sched_alloc_graph(ggml_backend_sched sched, ggml_cgraph graph); // returns success
    public static native @ByVal ggml_status ggml_backend_sched_graph_compute(ggml_backend_sched sched, ggml_cgraph graph);
    public static native @ByVal ggml_status ggml_backend_sched_graph_compute_async(ggml_backend_sched sched, ggml_cgraph graph);
    public static native void ggml_backend_sched_synchronize(ggml_backend_sched sched);

    // Reset all assignments and allocators - must be called before changing the node backends or allocating a new graph.
    // This in effect deallocates all tensors that were previously allocated and leaves them with dangling pointers.
    // The correct way to use this API is to discard the deallocated tensors and create new ones.
    public static native void ggml_backend_sched_reset(ggml_backend_sched sched);

    // Set a callback to be called for each resulting node during graph compute
    public static native void ggml_backend_sched_set_eval_callback(ggml_backend_sched sched, ggml_backend_sched_eval_callback callback, Pointer user_data);

    //
    // Utils
    //

    // Copy a graph to a different backend
    
// Targeting ../ggml_backend_eval_callback.java



    // Compare the output of two backends
    public static native @Cast("bool") boolean ggml_backend_compare_graph_backend(ggml_backend backend1, ggml_backend backend2, ggml_cgraph graph, ggml_backend_eval_callback callback, Pointer user_data);

    // Tensor initialization
    public static native @ByVal ggml_status ggml_backend_tensor_alloc(ggml_backend_buffer buffer, ggml_tensor tensor, Pointer addr);
    public static native @ByVal ggml_status ggml_backend_view_init(ggml_tensor tensor);

    // CPU buffer types are always available
    public static native ggml_backend_buffer ggml_backend_cpu_buffer_from_ptr(Pointer ptr, @Cast("size_t") long size);

// #ifdef  __cplusplus
// #endif


// Parsed from ggml.h

// #pragma once

//
// GGML Tensor Library
//
// This documentation is still a work in progress.
// If you wish some specific topics to be covered, feel free to drop a comment:
//
//   https://github.com/ggerganov/whisper.cpp/issues/40
//
// ## Overview
//
// This library implements:
//
//  - a set of tensor operations
//  - automatic differentiation
//  - basic optimization algorithms
//
// The aim of this library is to provide a minimalistic approach for various machine learning tasks. This includes,
// but is not limited to, the following:
//
//  - linear regression
//  - support vector machines
//  - neural networks
//
// The library allows the user to define a certain function using the available tensor operations. This function
// definition is represented internally via a computation graph. Each tensor operation in the function definition
// corresponds to a node in the graph. Having the computation graph defined, the user can choose to compute the
// function's value and/or its gradient with respect to the input variables. Optionally, the function can be optimized
// using one of the available optimization algorithms.
//
// For example, here we define the function: f(x) = a*x^2 + b
//
//   {
//       struct ggml_init_params params = {
//           .mem_size   = 16*1024*1024,
//           .mem_buffer = NULL,
//       };
//
//       // memory allocation happens here
//       struct ggml_context * ctx = ggml_init(params);
//
//       struct ggml_tensor * x = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
//
//       ggml_set_param(ctx, x); // x is an input variable
//
//       struct ggml_tensor * a  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
//       struct ggml_tensor * b  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
//       struct ggml_tensor * x2 = ggml_mul(ctx, x, x);
//       struct ggml_tensor * f  = ggml_add(ctx, ggml_mul(ctx, a, x2), b);
//
//       ...
//   }
//
// Notice that the function definition above does not involve any actual computation. The computation is performed only
// when the user explicitly requests it. For example, to compute the function's value at x = 2.0:
//
//   {
//       ...
//
//       struct ggml_cgraph * gf = ggml_new_graph(ctx);
//       ggml_build_forward_expand(gf, f);
//
//       // set the input variable and parameter values
//       ggml_set_f32(x, 2.0f);
//       ggml_set_f32(a, 3.0f);
//       ggml_set_f32(b, 4.0f);
//
//       ggml_graph_compute_with_ctx(ctx, &gf, n_threads);
//
//       printf("f = %f\n", ggml_get_f32_1d(f, 0));
//
//       ...
//   }
//
// The actual computation is performed in the ggml_graph_compute() function.
//
// The ggml_new_tensor_...() functions create new tensors. They are allocated in the memory buffer provided to the
// ggml_init() function. You have to be careful not to exceed the memory buffer size. Therefore, you have to know
// in advance how much memory you need for your computation. Alternatively, you can allocate a large enough memory
// and after defining the computation graph, call the ggml_used_mem() function to find out how much memory was
// actually needed.
//
// The ggml_set_param() function marks a tensor as an input variable. This is used by the automatic
// differentiation and optimization algorithms.
//
// The described approach allows to define the function graph once and then compute its forward or backward graphs
// multiple times. All computations will use the same memory buffer allocated in the ggml_init() function. This way
// the user can avoid the memory allocation overhead at runtime.
//
// The library supports multi-dimensional tensors - up to 4 dimensions. The FP16 and FP32 data types are first class
// citizens, but in theory the library can be extended to support FP8 and integer data types.
//
// Each tensor operation produces a new tensor. Initially the library was envisioned to support only the use of unary
// and binary operations. Most of the available operations fall into one of these two categories. With time, it became
// clear that the library needs to support more complex operations. The way to support these operations is not clear
// yet, but a few examples are demonstrated in the following operations:
//
//   - ggml_permute()
//   - ggml_conv_1d_1s()
//   - ggml_conv_1d_2s()
//
// For each tensor operator, the library implements a forward and backward computation function. The forward function
// computes the output tensor value given the input tensor values. The backward function computes the adjoint of the
// input tensors given the adjoint of the output tensor. For a detailed explanation of what this means, take a
// calculus class, or watch the following video:
//
//   What is Automatic Differentiation?
//   https://www.youtube.com/watch?v=wG_nF1awSSY
//
//
// ## Tensor data (struct ggml_tensor)
//
// The tensors are stored in memory via the ggml_tensor struct. The structure provides information about the size of
// the tensor, the data type, and the memory buffer where the tensor data is stored. Additionally, it contains
// pointers to the "source" tensors - i.e. the tensors that were used to compute the current tensor. For example:
//
//   {
//       struct ggml_tensor * c = ggml_add(ctx, a, b);
//
//       assert(c->src[0] == a);
//       assert(c->src[1] == b);
//   }
//
// The multi-dimensional tensors are stored in row-major order. The ggml_tensor struct contains fields for the
// number of elements in each dimension ("ne") as well as the number of bytes ("nb", a.k.a. stride). This allows
// to store tensors that are not contiguous in memory, which is useful for operations such as transposition and
// permutation. All tensor operations have to take the stride into account and not assume that the tensor is
// contiguous in memory.
//
// The data of the tensor is accessed via the "data" pointer. For example:
//
//   {
//       const int nx = 2;
//       const int ny = 3;
//
//       struct ggml_tensor * a = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, nx, ny);
//
//       for (int y = 0; y < ny; y++) {
//           for (int x = 0; x < nx; x++) {
//               *(float *) ((char *) a->data + y*a->nb[1] + x*a->nb[0]) = x + y;
//           }
//       }
//
//       ...
//   }
//
// Alternatively, there are helper functions, such as ggml_get_f32_1d() and ggml_set_f32_1d() that can be used.
//
// ## The matrix multiplication operator (ggml_mul_mat)
//
// TODO
//
//
// ## Multi-threading
//
// TODO
//
//
// ## Overview of ggml.c
//
// TODO
//
//
// ## SIMD optimizations
//
// TODO
//
//
// ## Debugging ggml
//
// TODO
//
//

// #ifdef GGML_SHARED
// #    if defined(_WIN32) && !defined(__MINGW32__)
// #        ifdef GGML_BUILD
// #            define GGML_API __declspec(dllexport) extern
// #        else
// #            define GGML_API __declspec(dllimport) extern
// #        endif
// #    else
// #        define GGML_API __attribute__ ((visibility ("default"))) extern
// #    endif
// #else
// #    define GGML_API extern
// #endif

// TODO: support for clang
// #ifdef __GNUC__
// #    define GGML_DEPRECATED(func, hint) func __attribute__((deprecated(hint)))
// #elif defined(_MSC_VER)
// #    define GGML_DEPRECATED(func, hint) __declspec(deprecated(hint)) func
// #else
// #    define GGML_DEPRECATED(func, hint) func
// #endif

// #ifndef __GNUC__
// #    define GGML_ATTRIBUTE_FORMAT(...)
// #elif defined(__MINGW32__) && !defined(__clang__)
// #    define GGML_ATTRIBUTE_FORMAT(...) __attribute__((format(gnu_printf, __VA_ARGS__)))
// #else
// #    define GGML_ATTRIBUTE_FORMAT(...) __attribute__((format(printf, __VA_ARGS__)))
// #endif

// #include <stdbool.h>
// #include <stddef.h>
// #include <stdint.h>
// #include <stdio.h>

public static final int GGML_FILE_MAGIC =   0x67676d6c; // "ggml"
public static final int GGML_FILE_VERSION = 2;

public static final int GGML_QNT_VERSION =        2;    // bump this on quantization format changes
public static final int GGML_QNT_VERSION_FACTOR = 1000; // do not change this

public static final int GGML_MAX_DIMS =           4;
public static final int GGML_MAX_PARAMS =         2048;
public static final int GGML_MAX_SRC =            10;
public static final int GGML_MAX_N_THREADS =      512;
public static final int GGML_MAX_OP_PARAMS =      64;

// #ifndef GGML_MAX_NAME
public static final int GGML_MAX_NAME =        64;
// #endif

public static final int GGML_DEFAULT_N_THREADS =  4;
public static final int GGML_DEFAULT_GRAPH_SIZE = 2048;

// #if UINTPTR_MAX == 0xFFFFFFFF
    public static final int GGML_MEM_ALIGN = 4;
// #else
// #endif

public static final int GGML_EXIT_SUCCESS = 0;
public static final int GGML_EXIT_ABORTED = 1;

public static final int GGML_ROPE_TYPE_NEOX =   2;
public static final int GGML_ROPE_TYPE_MROPE =  8;
public static final int GGML_ROPE_TYPE_VISION = 24;

// #define GGML_UNUSED(x) (void)(x)

// #define GGML_PAD(x, n) (((x) + (n) - 1) & ~((n) - 1))

// #ifndef NDEBUG
// #   define GGML_UNREACHABLE() do { fprintf(stderr, "statement should be unreachable\n"); abort(); } while(0)
// #elif defined(__GNUC__)
// #   define GGML_UNREACHABLE() __builtin_unreachable()
// #elif defined(_MSC_VER)
// #   define GGML_UNREACHABLE() __assume(0)
// #else
// #   define GGML_UNREACHABLE() ((void) 0)
// #endif

// #ifdef __cplusplus
// #   define GGML_NORETURN [[noreturn]]
// #elif defined(_MSC_VER)
// #   define GGML_NORETURN __declspec(noreturn)
// #else
// #   define GGML_NORETURN _Noreturn
// #endif

// #define GGML_ABORT(...) ggml_abort(__FILE__, __LINE__, __VA_ARGS__)
// #define GGML_ASSERT(x) if (!(x)) GGML_ABORT("GGML_ASSERT(%s) failed", #x)

// used to copy the number of elements and stride in bytes of tensors into local variables.
// main purpose is to reduce code duplication and improve readability.
//
// example:
//
//    GGML_TENSOR_LOCALS(int64_t, ne1, src1, ne);
//    GGML_TENSOR_LOCALS(size_t,  nb1, src1, nb);
//
// #define GGML_TENSOR_LOCALS_1(type, prefix, pointer, array)
//     const type prefix##0 = (pointer)->array[0];
//     GGML_UNUSED(prefix##0);
// #define GGML_TENSOR_LOCALS_2(type, prefix, pointer, array)
//     GGML_TENSOR_LOCALS_1    (type, prefix, pointer, array)
//     const type prefix##1 = (pointer)->array[1];
//     GGML_UNUSED(prefix##1);
// #define GGML_TENSOR_LOCALS_3(type, prefix, pointer, array)
//     GGML_TENSOR_LOCALS_2    (type, prefix, pointer, array)
//     const type prefix##2 = (pointer)->array[2];
//     GGML_UNUSED(prefix##2);
// #define GGML_TENSOR_LOCALS(type, prefix, pointer, array)
//     GGML_TENSOR_LOCALS_3  (type, prefix, pointer, array)
//     const type prefix##3 = (pointer)->array[3];
//     GGML_UNUSED(prefix##3);

// #define GGML_TENSOR_UNARY_OP_LOCALS
//     GGML_TENSOR_LOCALS(int64_t, ne0, src0, ne)
//     GGML_TENSOR_LOCALS(size_t,  nb0, src0, nb)
//     GGML_TENSOR_LOCALS(int64_t, ne,  dst,  ne)
//     GGML_TENSOR_LOCALS(size_t,  nb,  dst,  nb)

// #define GGML_TENSOR_BINARY_OP_LOCALS
//     GGML_TENSOR_LOCALS(int64_t, ne0, src0, ne)
//     GGML_TENSOR_LOCALS(size_t,  nb0, src0, nb)
//     GGML_TENSOR_LOCALS(int64_t, ne1, src1, ne)
//     GGML_TENSOR_LOCALS(size_t,  nb1, src1, nb)
//     GGML_TENSOR_LOCALS(int64_t, ne,  dst,  ne)
//     GGML_TENSOR_LOCALS(size_t,  nb,  dst,  nb)

// #define GGML_TENSOR_BINARY_OP_LOCALS01
//     GGML_TENSOR_LOCALS(int64_t, ne0, src0, ne)
//     GGML_TENSOR_LOCALS(size_t,  nb0, src0, nb)
//     GGML_TENSOR_LOCALS(int64_t, ne1, src1, ne)
//     GGML_TENSOR_LOCALS(size_t,  nb1, src1, nb)

// #ifdef  __cplusplus
// #endif

    public enum ggml_status {
        GGML_STATUS_ALLOC_FAILED(-2),
        GGML_STATUS_FAILED(-1),
        GGML_STATUS_SUCCESS(0),
        GGML_STATUS_ABORTED(1);

        public final int value;
        private ggml_status(int v) { this.value = v; }
        private ggml_status(ggml_status e) { this.value = e.value; }
        public ggml_status intern() { for (ggml_status e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }

    // get ggml_status name string
    public static native @Cast("const char*") BytePointer ggml_status_to_string(ggml_status status);
    public static native String ggml_status_to_string(@Cast("ggml_status") int status);

    // ieee 754-2008 half-precision float16
    // todo: make this not an integral type
    public static native float ggml_fp16_to_fp32(@Cast("ggml_fp16_t") short arg0);
    public static native @Cast("ggml_fp16_t") short ggml_fp32_to_fp16(float arg0);
    public static native void ggml_fp16_to_fp32_row(@Cast("const ggml_fp16_t*") ShortPointer arg0, FloatPointer arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp16_to_fp32_row(@Cast("const ggml_fp16_t*") ShortBuffer arg0, FloatBuffer arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp16_to_fp32_row(@Cast("const ggml_fp16_t*") short[] arg0, float[] arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp32_to_fp16_row(@Const FloatPointer arg0, @Cast("ggml_fp16_t*") ShortPointer arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp32_to_fp16_row(@Const FloatBuffer arg0, @Cast("ggml_fp16_t*") ShortBuffer arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp32_to_fp16_row(@Const float[] arg0, @Cast("ggml_fp16_t*") short[] arg1, @Cast("int64_t") long arg2);
// Targeting ../ggml_bf16_t.java


    public static native @ByVal ggml_bf16_t ggml_fp32_to_bf16(float arg0);
    public static native float ggml_bf16_to_fp32(@ByVal ggml_bf16_t arg0);  // consider just doing << 16
    public static native void ggml_bf16_to_fp32_row(@Const ggml_bf16_t arg0, FloatPointer arg1, @Cast("int64_t") long arg2);
    public static native void ggml_bf16_to_fp32_row(@Const ggml_bf16_t arg0, FloatBuffer arg1, @Cast("int64_t") long arg2);
    public static native void ggml_bf16_to_fp32_row(@Const ggml_bf16_t arg0, float[] arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp32_to_bf16_row_ref(@Const FloatPointer arg0, ggml_bf16_t arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp32_to_bf16_row_ref(@Const FloatBuffer arg0, ggml_bf16_t arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp32_to_bf16_row_ref(@Const float[] arg0, ggml_bf16_t arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp32_to_bf16_row(@Const FloatPointer arg0, ggml_bf16_t arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp32_to_bf16_row(@Const FloatBuffer arg0, ggml_bf16_t arg1, @Cast("int64_t") long arg2);
    public static native void ggml_fp32_to_bf16_row(@Const float[] arg0, ggml_bf16_t arg1, @Cast("int64_t") long arg2);
// Targeting ../ggml_object.java


// Targeting ../ggml_context.java


// Targeting ../ggml_cgraph.java



    // NOTE: always add types at the end of the enum to keep backward compatibility
    public enum ggml_type {
        GGML_TYPE_F32    (0),
        GGML_TYPE_F16    (1),
        GGML_TYPE_Q4_0   (2),
        GGML_TYPE_Q4_1   (3),
        // GGML_TYPE_Q4_2 = 4, support has been removed
        // GGML_TYPE_Q4_3 = 5, support has been removed
        GGML_TYPE_Q5_0   (6),
        GGML_TYPE_Q5_1   (7),
        GGML_TYPE_Q8_0   (8),
        GGML_TYPE_Q8_1   (9),
        GGML_TYPE_Q2_K   (10),
        GGML_TYPE_Q3_K   (11),
        GGML_TYPE_Q4_K   (12),
        GGML_TYPE_Q5_K   (13),
        GGML_TYPE_Q6_K   (14),
        GGML_TYPE_Q8_K   (15),
        GGML_TYPE_IQ2_XXS(16),
        GGML_TYPE_IQ2_XS (17),
        GGML_TYPE_IQ3_XXS(18),
        GGML_TYPE_IQ1_S  (19),
        GGML_TYPE_IQ4_NL (20),
        GGML_TYPE_IQ3_S  (21),
        GGML_TYPE_IQ2_S  (22),
        GGML_TYPE_IQ4_XS (23),
        GGML_TYPE_I8     (24),
        GGML_TYPE_I16    (25),
        GGML_TYPE_I32    (26),
        GGML_TYPE_I64    (27),
        GGML_TYPE_F64    (28),
        GGML_TYPE_IQ1_M  (29),
        GGML_TYPE_BF16   (30),
        // GGML_TYPE_Q4_0_4_4 = 31, support has been removed from gguf files
        // GGML_TYPE_Q4_0_4_8 = 32,
        // GGML_TYPE_Q4_0_8_8 = 33,
        GGML_TYPE_TQ1_0  (34),
        GGML_TYPE_TQ2_0  (35),
        // GGML_TYPE_IQ4_NL_4_4 = 36,
        // GGML_TYPE_IQ4_NL_4_8 = 37,
        // GGML_TYPE_IQ4_NL_8_8 = 38,
        GGML_TYPE_COUNT  (39);

        public final int value;
        private ggml_type(int v) { this.value = v; }
        private ggml_type(ggml_type e) { this.value = e.value; }
        public ggml_type intern() { for (ggml_type e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }

    // precision
    public enum ggml_prec {
        GGML_PREC_DEFAULT(0),
        GGML_PREC_F32(1);

        public final int value;
        private ggml_prec(int v) { this.value = v; }
        private ggml_prec(ggml_prec e) { this.value = e.value; }
        public ggml_prec intern() { for (ggml_prec e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }

    // model file types
    public enum ggml_ftype {
        GGML_FTYPE_UNKNOWN       (-1),
        GGML_FTYPE_ALL_F32       (0),
        GGML_FTYPE_MOSTLY_F16    (1),  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_0   (2),  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_1   (3),  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_1_SOME_F16(4), // tok_embeddings.weight and output.weight are F16
        GGML_FTYPE_MOSTLY_Q8_0   (7),  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q5_0   (8),  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q5_1   (9),  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q2_K   (10), // except 1d tensors
        GGML_FTYPE_MOSTLY_Q3_K   (11), // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_K   (12), // except 1d tensors
        GGML_FTYPE_MOSTLY_Q5_K   (13), // except 1d tensors
        GGML_FTYPE_MOSTLY_Q6_K   (14), // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ2_XXS(15), // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ2_XS (16), // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ3_XXS(17), // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ1_S  (18), // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ4_NL (19), // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ3_S  (20), // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ2_S  (21), // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ4_XS (22), // except 1d tensors
        GGML_FTYPE_MOSTLY_IQ1_M  (23), // except 1d tensors
        GGML_FTYPE_MOSTLY_BF16   (24);// except 1d tensors

        public final int value;
        private ggml_ftype(int v) { this.value = v; }
        private ggml_ftype(ggml_ftype e) { this.value = e.value; }
        public ggml_ftype intern() { for (ggml_ftype e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }

    // available tensor operations:
    public enum ggml_op {
        GGML_OP_NONE(0),

        GGML_OP_DUP(1),
        GGML_OP_ADD(2),
        GGML_OP_ADD1(3),
        GGML_OP_ACC(4),
        GGML_OP_SUB(5),
        GGML_OP_MUL(6),
        GGML_OP_DIV(7),
        GGML_OP_SQR(8),
        GGML_OP_SQRT(9),
        GGML_OP_LOG(10),
        GGML_OP_SIN(11),
        GGML_OP_COS(12),
        GGML_OP_SUM(13),
        GGML_OP_SUM_ROWS(14),
        GGML_OP_MEAN(15),
        GGML_OP_ARGMAX(16),
        GGML_OP_COUNT_EQUAL(17),
        GGML_OP_REPEAT(18),
        GGML_OP_REPEAT_BACK(19),
        GGML_OP_CONCAT(20),
        GGML_OP_SILU_BACK(21),
        GGML_OP_NORM(22), // normalize
        GGML_OP_RMS_NORM(23),
        GGML_OP_RMS_NORM_BACK(24),
        GGML_OP_GROUP_NORM(25),

        GGML_OP_MUL_MAT(26),
        GGML_OP_MUL_MAT_ID(27),
        GGML_OP_OUT_PROD(28),

        GGML_OP_SCALE(29),
        GGML_OP_SET(30),
        GGML_OP_CPY(31),
        GGML_OP_CONT(32),
        GGML_OP_RESHAPE(33),
        GGML_OP_VIEW(34),
        GGML_OP_PERMUTE(35),
        GGML_OP_TRANSPOSE(36),
        GGML_OP_GET_ROWS(37),
        GGML_OP_GET_ROWS_BACK(38),
        GGML_OP_DIAG(39),
        GGML_OP_DIAG_MASK_INF(40),
        GGML_OP_DIAG_MASK_ZERO(41),
        GGML_OP_SOFT_MAX(42),
        GGML_OP_SOFT_MAX_BACK(43),
        GGML_OP_ROPE(44),
        GGML_OP_ROPE_BACK(45),
        GGML_OP_CLAMP(46),
        GGML_OP_CONV_TRANSPOSE_1D(47),
        GGML_OP_IM2COL(48),
        GGML_OP_IM2COL_BACK(49),
        GGML_OP_CONV_TRANSPOSE_2D(50),
        GGML_OP_POOL_1D(51),
        GGML_OP_POOL_2D(52),
        GGML_OP_POOL_2D_BACK(53),
        GGML_OP_UPSCALE(54), // nearest interpolate
        GGML_OP_PAD(55),
        GGML_OP_PAD_REFLECT_1D(56),
        GGML_OP_ARANGE(57),
        GGML_OP_TIMESTEP_EMBEDDING(58),
        GGML_OP_ARGSORT(59),
        GGML_OP_LEAKY_RELU(60),

        GGML_OP_FLASH_ATTN_EXT(61),
        GGML_OP_FLASH_ATTN_BACK(62),
        GGML_OP_SSM_CONV(63),
        GGML_OP_SSM_SCAN(64),
        GGML_OP_WIN_PART(65),
        GGML_OP_WIN_UNPART(66),
        GGML_OP_GET_REL_POS(67),
        GGML_OP_ADD_REL_POS(68),
        GGML_OP_RWKV_WKV6(69),
        GGML_OP_GATED_LINEAR_ATTN(70),

        GGML_OP_UNARY(71),

        GGML_OP_MAP_UNARY(72),
        GGML_OP_MAP_BINARY(73),

        GGML_OP_MAP_CUSTOM1_F32(74),
        GGML_OP_MAP_CUSTOM2_F32(75),
        GGML_OP_MAP_CUSTOM3_F32(76),

        GGML_OP_MAP_CUSTOM1(77),
        GGML_OP_MAP_CUSTOM2(78),
        GGML_OP_MAP_CUSTOM3(79),

        GGML_OP_CROSS_ENTROPY_LOSS(80),
        GGML_OP_CROSS_ENTROPY_LOSS_BACK(81),
        GGML_OP_OPT_STEP_ADAMW(82),

        GGML_OP_COUNT(83);

        public final int value;
        private ggml_op(int v) { this.value = v; }
        private ggml_op(ggml_op e) { this.value = e.value; }
        public ggml_op intern() { for (ggml_op e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }

    public enum ggml_unary_op {
        GGML_UNARY_OP_ABS(0),
        GGML_UNARY_OP_SGN(1),
        GGML_UNARY_OP_NEG(2),
        GGML_UNARY_OP_STEP(3),
        GGML_UNARY_OP_TANH(4),
        GGML_UNARY_OP_ELU(5),
        GGML_UNARY_OP_RELU(6),
        GGML_UNARY_OP_SIGMOID(7),
        GGML_UNARY_OP_GELU(8),
        GGML_UNARY_OP_GELU_QUICK(9),
        GGML_UNARY_OP_SILU(10),
        GGML_UNARY_OP_HARDSWISH(11),
        GGML_UNARY_OP_HARDSIGMOID(12),
        GGML_UNARY_OP_EXP(13),

        GGML_UNARY_OP_COUNT(14);

        public final int value;
        private ggml_unary_op(int v) { this.value = v; }
        private ggml_unary_op(ggml_unary_op e) { this.value = e.value; }
        public ggml_unary_op intern() { for (ggml_unary_op e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }

    public enum ggml_object_type {
        GGML_OBJECT_TYPE_TENSOR(0),
        GGML_OBJECT_TYPE_GRAPH(1),
        GGML_OBJECT_TYPE_WORK_BUFFER(2);

        public final int value;
        private ggml_object_type(int v) { this.value = v; }
        private ggml_object_type(ggml_object_type e) { this.value = e.value; }
        public ggml_object_type intern() { for (ggml_object_type e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }

    public enum ggml_log_level {
        GGML_LOG_LEVEL_NONE (0),
        GGML_LOG_LEVEL_DEBUG(1),
        GGML_LOG_LEVEL_INFO (2),
        GGML_LOG_LEVEL_WARN (3),
        GGML_LOG_LEVEL_ERROR(4),
        GGML_LOG_LEVEL_CONT (5);// continue previous log

        public final int value;
        private ggml_log_level(int v) { this.value = v; }
        private ggml_log_level(ggml_log_level e) { this.value = e.value; }
        public ggml_log_level intern() { for (ggml_log_level e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }

    // this tensor...
    public enum ggml_tensor_flag {
        GGML_TENSOR_FLAG_INPUT (1), // ...is an input for the GGML compute graph
        GGML_TENSOR_FLAG_OUTPUT(2), // ...is an output for the GGML compute graph
        GGML_TENSOR_FLAG_PARAM (4), // ...contains trainable parameters
        GGML_TENSOR_FLAG_LOSS  (8);// ...defines loss for numerical optimization (multiple loss tensors add up)

        public final int value;
        private ggml_tensor_flag(int v) { this.value = v; }
        private ggml_tensor_flag(ggml_tensor_flag e) { this.value = e.value; }
        public ggml_tensor_flag intern() { for (ggml_tensor_flag e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }
// Targeting ../ggml_init_params.java


// Targeting ../ggml_tensor.java



    @MemberGetter public static native @Cast("const size_t") long GGML_TENSOR_SIZE();
    public static final long GGML_TENSOR_SIZE = GGML_TENSOR_SIZE();

    // Abort callback
    // If not NULL, called before ggml computation
    // If it returns true, the computation is aborted


    //
    // GUID
    //

    // GUID types

    // misc

    public static native void ggml_time_init(); // call this once at the beginning of the program
    public static native @Cast("int64_t") long ggml_time_ms();
    public static native @Cast("int64_t") long ggml_time_us();
    public static native @Cast("int64_t") long ggml_cycles();
    public static native @Cast("int64_t") long ggml_cycles_per_ms();

    // accepts a UTF-8 path, even on Windows
    public static native @Cast("FILE*") Pointer ggml_fopen(@Cast("const char*") BytePointer fname, @Cast("const char*") BytePointer mode);
    public static native @Cast("FILE*") Pointer ggml_fopen(String fname, String mode);

    public static native void ggml_print_object(@Const ggml_object obj);
    public static native void ggml_print_objects(@Const ggml_context ctx);

    public static native @Cast("int64_t") long ggml_nelements(@Const ggml_tensor tensor);
    public static native @Cast("int64_t") long ggml_nrows(@Const ggml_tensor tensor);
    public static native @Cast("size_t") long ggml_nbytes(@Const ggml_tensor tensor);
    public static native @Cast("size_t") long ggml_nbytes_pad(@Const ggml_tensor tensor); // same as ggml_nbytes() but padded to GGML_MEM_ALIGN

    public static native @Cast("int64_t") long ggml_blck_size(ggml_type type);
    public static native @Cast("int64_t") long ggml_blck_size(@Cast("ggml_type") int type);
    public static native @Cast("size_t") long ggml_type_size(ggml_type type);
    public static native @Cast("size_t") long ggml_type_size(@Cast("ggml_type") int type);             // size in bytes for all elements in a block
    public static native @Cast("size_t") long ggml_row_size(ggml_type type, @Cast("int64_t") long ne);
    public static native @Cast("size_t") long ggml_row_size(@Cast("ggml_type") int type, @Cast("int64_t") long ne); // size in bytes for all elements in a row

    public static native double ggml_type_sizef(ggml_type type);
    public static native double ggml_type_sizef(@Cast("ggml_type") int type);

    public static native @Cast("const char*") BytePointer ggml_type_name(ggml_type type);
    public static native String ggml_type_name(@Cast("ggml_type") int type);
    public static native @Cast("const char*") BytePointer ggml_op_name(ggml_op op);
    public static native String ggml_op_name(@Cast("ggml_op") int op);
    public static native @Cast("const char*") BytePointer ggml_op_symbol(ggml_op op);
    public static native String ggml_op_symbol(@Cast("ggml_op") int op);

    public static native @Cast("const char*") BytePointer ggml_unary_op_name(ggml_unary_op op);
    public static native String ggml_unary_op_name(@Cast("ggml_unary_op") int op);
    public static native @Cast("const char*") BytePointer ggml_op_desc(@Const ggml_tensor t); // unary or op name

    public static native @Cast("size_t") long ggml_element_size(@Const ggml_tensor tensor);

    public static native @Cast("bool") boolean ggml_is_quantized(ggml_type type);
    public static native @Cast("bool") boolean ggml_is_quantized(@Cast("ggml_type") int type);

    // TODO: temporary until model loading of ggml examples is refactored
    public static native ggml_type ggml_ftype_to_ggml_type(ggml_ftype ftype);
    public static native @Cast("ggml_type") int ggml_ftype_to_ggml_type(@Cast("ggml_ftype") int ftype);

    public static native @Cast("bool") boolean ggml_is_transposed(@Const ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_is_permuted(@Const ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_is_empty(@Const ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_is_scalar(@Const ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_is_vector(@Const ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_is_matrix(@Const ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_is_3d(@Const ggml_tensor tensor);
    public static native int ggml_n_dims(@Const ggml_tensor tensor); // returns 1 for scalars

    public static native @Cast("bool") boolean ggml_is_contiguous(@Const ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_is_contiguous_0(@Const ggml_tensor tensor); // same as ggml_is_contiguous()
    public static native @Cast("bool") boolean ggml_is_contiguous_1(@Const ggml_tensor tensor); // contiguous for dims >= 1
    public static native @Cast("bool") boolean ggml_is_contiguous_2(@Const ggml_tensor tensor); // contiguous for dims >= 2

    public static native @Cast("bool") boolean ggml_are_same_shape(@Const ggml_tensor t0, @Const ggml_tensor t1);
    public static native @Cast("bool") boolean ggml_are_same_stride(@Const ggml_tensor t0, @Const ggml_tensor t1);

    public static native @Cast("bool") boolean ggml_can_repeat(@Const ggml_tensor t0, @Const ggml_tensor t1);

    // use this to compute the memory overhead of a tensor
    public static native @Cast("size_t") long ggml_tensor_overhead();

    public static native @Cast("bool") boolean ggml_validate_row_data(ggml_type type, @Const Pointer data, @Cast("size_t") long nbytes);
    public static native @Cast("bool") boolean ggml_validate_row_data(@Cast("ggml_type") int type, @Const Pointer data, @Cast("size_t") long nbytes);

    // main

    public static native ggml_context ggml_init(@ByVal ggml_init_params params);
    public static native void ggml_reset(ggml_context ctx);
    public static native void ggml_free(ggml_context ctx);

    public static native @Cast("size_t") long ggml_used_mem(@Const ggml_context ctx);

    public static native @Cast("bool") boolean ggml_get_no_alloc(ggml_context ctx);
    public static native void ggml_set_no_alloc(ggml_context ctx, @Cast("bool") boolean no_alloc);

    public static native Pointer ggml_get_mem_buffer(@Const ggml_context ctx);
    public static native @Cast("size_t") long ggml_get_mem_size(@Const ggml_context ctx);
    public static native @Cast("size_t") long ggml_get_max_tensor_size(@Const ggml_context ctx);

    public static native ggml_tensor ggml_new_tensor(
                ggml_context ctx,
                ggml_type type,
                int n_dims,
                @Cast("const int64_t*") LongPointer ne);
    public static native ggml_tensor ggml_new_tensor(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                int n_dims,
                @Cast("const int64_t*") LongBuffer ne);
    public static native ggml_tensor ggml_new_tensor(
                ggml_context ctx,
                ggml_type type,
                int n_dims,
                @Cast("const int64_t*") long[] ne);
    public static native ggml_tensor ggml_new_tensor(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                int n_dims,
                @Cast("const int64_t*") LongPointer ne);
    public static native ggml_tensor ggml_new_tensor(
                ggml_context ctx,
                ggml_type type,
                int n_dims,
                @Cast("const int64_t*") LongBuffer ne);
    public static native ggml_tensor ggml_new_tensor(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                int n_dims,
                @Cast("const int64_t*") long[] ne);

    public static native ggml_tensor ggml_new_tensor_1d(
                ggml_context ctx,
                ggml_type type,
                @Cast("int64_t") long ne0);
    public static native ggml_tensor ggml_new_tensor_1d(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                @Cast("int64_t") long ne0);

    public static native ggml_tensor ggml_new_tensor_2d(
                ggml_context ctx,
                ggml_type type,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1);
    public static native ggml_tensor ggml_new_tensor_2d(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1);

    public static native ggml_tensor ggml_new_tensor_3d(
                ggml_context ctx,
                ggml_type type,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2);
    public static native ggml_tensor ggml_new_tensor_3d(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2);

    public static native ggml_tensor ggml_new_tensor_4d(
                ggml_context ctx,
                ggml_type type,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("int64_t") long ne3);
    public static native ggml_tensor ggml_new_tensor_4d(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("int64_t") long ne3);

    public static native Pointer ggml_new_buffer(ggml_context ctx, @Cast("size_t") long nbytes);

    public static native ggml_tensor ggml_dup_tensor(ggml_context ctx, @Const ggml_tensor src);
    public static native ggml_tensor ggml_view_tensor(ggml_context ctx, ggml_tensor src);

    // Context tensor enumeration and lookup
    public static native ggml_tensor ggml_get_first_tensor(@Const ggml_context ctx);
    public static native ggml_tensor ggml_get_next_tensor(@Const ggml_context ctx, ggml_tensor tensor);
    public static native ggml_tensor ggml_get_tensor(ggml_context ctx, @Cast("const char*") BytePointer name);
    public static native ggml_tensor ggml_get_tensor(ggml_context ctx, String name);

    // Converts a flat index into coordinates
    public static native void ggml_unravel_index(@Const ggml_tensor tensor, @Cast("int64_t") long i, @Cast("int64_t*") LongPointer i0, @Cast("int64_t*") LongPointer i1, @Cast("int64_t*") LongPointer i2, @Cast("int64_t*") LongPointer i3);
    public static native void ggml_unravel_index(@Const ggml_tensor tensor, @Cast("int64_t") long i, @Cast("int64_t*") LongBuffer i0, @Cast("int64_t*") LongBuffer i1, @Cast("int64_t*") LongBuffer i2, @Cast("int64_t*") LongBuffer i3);
    public static native void ggml_unravel_index(@Const ggml_tensor tensor, @Cast("int64_t") long i, @Cast("int64_t*") long[] i0, @Cast("int64_t*") long[] i1, @Cast("int64_t*") long[] i2, @Cast("int64_t*") long[] i3);

    public static native ggml_unary_op ggml_get_unary_op(@Const ggml_tensor tensor);

    public static native Pointer ggml_get_data(@Const ggml_tensor tensor);
    public static native FloatPointer ggml_get_data_f32(@Const ggml_tensor tensor);

    public static native @Cast("const char*") BytePointer ggml_get_name(@Const ggml_tensor tensor);
    public static native ggml_tensor ggml_set_name(      ggml_tensor tensor, @Cast("const char*") BytePointer name);
    public static native ggml_tensor ggml_set_name(      ggml_tensor tensor, String name);
    public static native ggml_tensor ggml_format_name(      ggml_tensor tensor, @Cast("const char*") BytePointer fmt);
    public static native ggml_tensor ggml_format_name(      ggml_tensor tensor, String fmt);

    // Tensor flags
    public static native void ggml_set_input(ggml_tensor tensor);
    public static native void ggml_set_output(ggml_tensor tensor);
    public static native void ggml_set_param(ggml_context ctx, ggml_tensor tensor);
    public static native void ggml_set_loss(ggml_tensor tensor);

    //
    // operations on tensors with backpropagation
    //

    public static native ggml_tensor ggml_dup(
                ggml_context ctx,
                ggml_tensor a);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_dup_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_add(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_add_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_add_cast(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_type type);
    public static native ggml_tensor ggml_add_cast(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("ggml_type") int type);

    public static native ggml_tensor ggml_add1(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_add1_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // dst = a
    // view(dst, nb1, nb2, nb3, offset) += b
    // return dst
    public static native ggml_tensor ggml_acc(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_acc_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_sub(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_sub_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_mul(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_mul_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_div(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_div_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_sqr(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sqr_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sqrt(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sqrt_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_log(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_log_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sin(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sin_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_cos(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_cos_inplace(
                ggml_context ctx,
                ggml_tensor a);

    // return scalar
    public static native ggml_tensor ggml_sum(
                ggml_context ctx,
                ggml_tensor a);

    // sums along rows, with input shape [a,b,c,d] return shape [1,b,c,d]
    public static native ggml_tensor ggml_sum_rows(
                ggml_context ctx,
                ggml_tensor a);

    // mean along rows
    public static native ggml_tensor ggml_mean(
                ggml_context ctx,
                ggml_tensor a);

    // argmax along rows
    public static native ggml_tensor ggml_argmax(
                ggml_context ctx,
                ggml_tensor a);

    // count number of equal elements in a and b
    public static native ggml_tensor ggml_count_equal(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // if a is the same shape as b, and a is not parameter, return a
    // otherwise, return a new tensor: repeat(a) to fit in b
    public static native ggml_tensor ggml_repeat(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // sums repetitions in a into shape of b
    public static native ggml_tensor ggml_repeat_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // concat a and b along dim
    // used in stable-diffusion
    public static native ggml_tensor ggml_concat(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int dim);

    public static native ggml_tensor ggml_abs(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_abs_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sgn(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sgn_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_neg(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_neg_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_step(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_step_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_tanh(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_tanh_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_elu(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_elu_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_relu(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_leaky_relu(
                ggml_context ctx,
                ggml_tensor a, float negative_slope, @Cast("bool") boolean inplace);

    public static native ggml_tensor ggml_relu_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sigmoid(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sigmoid_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_gelu(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_gelu_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_gelu_quick(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_gelu_quick_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_silu(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_silu_inplace(
                ggml_context ctx,
                ggml_tensor a);

    // a - x
    // b - dy
    public static native ggml_tensor ggml_silu_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // hardswish(x) = x * relu6(x + 3) / 6
    public static native ggml_tensor ggml_hardswish(
                ggml_context ctx,
                ggml_tensor a);

    // hardsigmoid(x) = relu6(x + 3) / 6
    public static native ggml_tensor ggml_hardsigmoid(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_exp(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_exp_inplace(
                ggml_context ctx,
                ggml_tensor a);

    // normalize along rows
    public static native ggml_tensor ggml_norm(
                ggml_context ctx,
                ggml_tensor a,
                float eps);

    public static native ggml_tensor ggml_norm_inplace(
                ggml_context ctx,
                ggml_tensor a,
                float eps);

    public static native ggml_tensor ggml_rms_norm(
                ggml_context ctx,
                ggml_tensor a,
                float eps);

    public static native ggml_tensor ggml_rms_norm_inplace(
                ggml_context ctx,
                ggml_tensor a,
                float eps);

    // group normalize along ne0*ne1*n_groups
    // used in stable-diffusion
    public static native ggml_tensor ggml_group_norm(
                ggml_context ctx,
                ggml_tensor a,
                int n_groups,
                float eps);

    public static native ggml_tensor ggml_group_norm_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_groups,
                float eps);

    // a - x
    // b - dy
    public static native ggml_tensor ggml_rms_norm_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                float eps);

    // A: k columns, n rows => [ne03, ne02, n, k]
    // B: k columns, m rows  (i.e. we transpose it internally) => [ne03 * x, ne02 * y, m, k]
    // result is n columns, m rows => [ne03 * x, ne02 * y, m, n]
    public static native ggml_tensor ggml_mul_mat(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // change the precision of a matrix multiplication
    // set to GGML_PREC_F32 for higher precision (useful for phi-2)
    public static native void ggml_mul_mat_set_prec(
                ggml_tensor a,
                ggml_prec prec);
    public static native void ggml_mul_mat_set_prec(
                ggml_tensor a,
                @Cast("ggml_prec") int prec);

    // indirect matrix multiplication
    public static native ggml_tensor ggml_mul_mat_id(
                ggml_context ctx,
                ggml_tensor as,
                ggml_tensor b,
                ggml_tensor ids);

    // A: m columns, n rows,
    // B: p columns, n rows,
    // result is m columns, p rows
    public static native ggml_tensor ggml_out_prod(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    //
    // operations on tensors without backpropagation
    //

    public static native ggml_tensor ggml_scale(
                ggml_context ctx,
                ggml_tensor a,
                float s);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_scale_inplace(
                ggml_context ctx,
                ggml_tensor a,
                float s);

    // b -> view(a,offset,nb1,nb2,3), return modified a
    public static native ggml_tensor ggml_set(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset); // in bytes

    // b -> view(a,offset,nb1,nb2,3), return view(a)
    public static native ggml_tensor ggml_set_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset); // in bytes

    public static native ggml_tensor ggml_set_1d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long offset); // in bytes

    public static native ggml_tensor ggml_set_1d_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long offset); // in bytes

    // b -> view(a,offset,nb1,nb2,3), return modified a
    public static native ggml_tensor ggml_set_2d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long offset); // in bytes

    // b -> view(a,offset,nb1,nb2,3), return view(a)
    public static native ggml_tensor ggml_set_2d_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long offset); // in bytes

    // a -> b, return view(b)
    public static native ggml_tensor ggml_cpy(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_cast(
                ggml_context ctx,
                ggml_tensor a,
                ggml_type type);
    public static native ggml_tensor ggml_cast(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("ggml_type") int type);

    // make contiguous
    public static native ggml_tensor ggml_cont(
                ggml_context ctx,
                ggml_tensor a);

    // make contiguous, with new shape
    public static native ggml_tensor ggml_cont_1d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0);

    public static native ggml_tensor ggml_cont_2d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1);

    public static native ggml_tensor ggml_cont_3d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2);

    public static native ggml_tensor ggml_cont_4d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("int64_t") long ne3);

    // return view(a), b specifies the new shape
    // TODO: when we start computing gradient, make a copy instead of view
    public static native ggml_tensor ggml_reshape(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // return view(a)
    // TODO: when we start computing gradient, make a copy instead of view
    public static native ggml_tensor ggml_reshape_1d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0);

    public static native ggml_tensor ggml_reshape_2d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1);

    // return view(a)
    // TODO: when we start computing gradient, make a copy instead of view
    public static native ggml_tensor ggml_reshape_3d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2);

    public static native ggml_tensor ggml_reshape_4d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("int64_t") long ne3);

    // offset in bytes
    public static native ggml_tensor ggml_view_1d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_view_2d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("size_t") long nb1,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_view_3d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_view_4d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("int64_t") long ne3,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_permute(
                ggml_context ctx,
                ggml_tensor a,
                int axis0,
                int axis1,
                int axis2,
                int axis3);

    // alias for ggml_permute(ctx, a, 1, 0, 2, 3)
    public static native ggml_tensor ggml_transpose(
                ggml_context ctx,
                ggml_tensor a);

    // supports 3D: a->ne[2] == b->ne[1]
    public static native ggml_tensor ggml_get_rows(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b); // row indices

    public static native ggml_tensor ggml_get_rows_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c); // data for ggml_get_rows, only used for its shape

    public static native ggml_tensor ggml_diag(
            ggml_context ctx,
            ggml_tensor a);

    // set elements above the diagonal to -INF
    public static native ggml_tensor ggml_diag_mask_inf(
                ggml_context ctx,
                ggml_tensor a,
                int n_past);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_diag_mask_inf_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_past);

    // set elements above the diagonal to 0
    public static native ggml_tensor ggml_diag_mask_zero(
                ggml_context ctx,
                ggml_tensor a,
                int n_past);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_diag_mask_zero_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_past);

    public static native ggml_tensor ggml_soft_max(
                ggml_context ctx,
                ggml_tensor a);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_soft_max_inplace(
                ggml_context ctx,
                ggml_tensor a);

    // fused soft_max(a*scale + mask*(ALiBi slope))
    // mask is optional
    // max_bias = 0.0f for no ALiBi
    public static native ggml_tensor ggml_soft_max_ext(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor mask,
                float scale,
                float max_bias);

    public static native ggml_tensor ggml_soft_max_ext_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                float scale,
                float max_bias);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_soft_max_ext_back_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                float scale,
                float max_bias);

    // rotary position embedding
    // if (mode & 1) - skip n_past elements (NOT SUPPORTED)
    // if (mode & GGML_ROPE_TYPE_NEOX) - GPT-NeoX style
    //
    // b is an int32 vector with size a->ne[2], it contains the positions
    public static native ggml_tensor ggml_rope(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int n_dims,
                int mode);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_rope_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int n_dims,
                int mode);

    // custom RoPE
    // c is freq factors (e.g. phi3-128k), (optional)
    public static native ggml_tensor ggml_rope_ext(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                int n_dims,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);

    public static native ggml_tensor ggml_rope_multi(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                int n_dims,
                IntPointer sections,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);
    public static native ggml_tensor ggml_rope_multi(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                int n_dims,
                IntBuffer sections,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);
    public static native ggml_tensor ggml_rope_multi(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                int n_dims,
                int[] sections,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_rope_ext_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                int n_dims,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);

    public static native ggml_tensor ggml_rope_custom(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int n_dims,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);

    public static native ggml_tensor ggml_rope_custom_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int n_dims,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);

    // compute correction dims for YaRN RoPE scaling
    public static native void ggml_rope_yarn_corr_dims(
            int n_dims, int n_ctx_orig, float freq_base, float beta_fast, float beta_slow, FloatPointer dims);
    public static native void ggml_rope_yarn_corr_dims(
            int n_dims, int n_ctx_orig, float freq_base, float beta_fast, float beta_slow, FloatBuffer dims);
    public static native void ggml_rope_yarn_corr_dims(
            int n_dims, int n_ctx_orig, float freq_base, float beta_fast, float beta_slow, float[] dims);

    // rotary position embedding backward, i.e compute dx from dy
    // a - dy
    public static native ggml_tensor ggml_rope_ext_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                int n_dims,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);

    public static native ggml_tensor ggml_rope_multi_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                int n_dims,
                IntPointer sections,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);
    public static native ggml_tensor ggml_rope_multi_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                int n_dims,
                IntBuffer sections,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);
    public static native ggml_tensor ggml_rope_multi_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                int n_dims,
                int[] sections,
                int mode,
                int n_ctx_orig,
                float freq_base,
                float freq_scale,
                float ext_factor,
                float attn_factor,
                float beta_fast,
                float beta_slow);


    // clamp
    // in-place, returns view(a)
    public static native ggml_tensor ggml_clamp(
                ggml_context ctx,
                ggml_tensor a,
                float min,
                float max);

    // im2col
    // converts data into a format that effectively results in a convolution when combined with matrix multiplication
    public static native ggml_tensor ggml_im2col(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int s1,
                int p0,
                int p1,
                int d0,
                int d1,
                @Cast("bool") boolean is_2D,
                ggml_type dst_type);
    public static native ggml_tensor ggml_im2col(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int s1,
                int p0,
                int p1,
                int d0,
                int d1,
                @Cast("bool") boolean is_2D,
                @Cast("ggml_type") int dst_type);

    public static native ggml_tensor ggml_im2col_back(
            ggml_context ctx,
            ggml_tensor a,
            ggml_tensor b,
            @Cast("int64_t*") LongPointer ne,
            int s0,
            int s1,
            int p0,
            int p1,
            int d0,
            int d1,
            @Cast("bool") boolean is_2D);
    public static native ggml_tensor ggml_im2col_back(
            ggml_context ctx,
            ggml_tensor a,
            ggml_tensor b,
            @Cast("int64_t*") LongBuffer ne,
            int s0,
            int s1,
            int p0,
            int p1,
            int d0,
            int d1,
            @Cast("bool") boolean is_2D);
    public static native ggml_tensor ggml_im2col_back(
            ggml_context ctx,
            ggml_tensor a,
            ggml_tensor b,
            @Cast("int64_t*") long[] ne,
            int s0,
            int s1,
            int p0,
            int p1,
            int d0,
            int d1,
            @Cast("bool") boolean is_2D);

    public static native ggml_tensor ggml_conv_1d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int p0,
                int d0); // dilation

    // conv_1d with padding = half
    // alias for ggml_conv_1d(a, b, s, a->ne[0]/2, d)
    public static native ggml_tensor ggml_conv_1d_ph(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s,
                int d); // dilation

    // depthwise
    // TODO: this is very likely wrong for some cases! - needs more testing
    public static native ggml_tensor ggml_conv_1d_dw(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int p0,
                int d0); // dilation

    public static native ggml_tensor ggml_conv_1d_dw_ph(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int d0); // dilation

    public static native ggml_tensor ggml_conv_transpose_1d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int p0,
                int d0); // dilation

    public static native ggml_tensor ggml_conv_2d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int s1,
                int p0,
                int p1,
                int d0,
                int d1); // dilation dimension 1

    // kernel size is a->ne[0] x a->ne[1]
    // stride is equal to kernel size
    // padding is zero
    // example:
    // a:     16   16    3  768
    // b:   1024 1024    3    1
    // res:   64   64  768    1
    // used in sam
    public static native ggml_tensor ggml_conv_2d_sk_p0(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // kernel size is a->ne[0] x a->ne[1]
    // stride is 1
    // padding is half
    // example:
    // a:      3    3    256  256
    // b:     64   64    256    1
    // res:   64   64    256    1
    // used in sam
    public static native ggml_tensor ggml_conv_2d_s1_ph(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // depthwise
    public static native ggml_tensor ggml_conv_2d_dw(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int s1,
                int p0,
                int p1,
                int d0,
                int d1); // dilation dimension 1

    public static native ggml_tensor ggml_conv_transpose_2d_p0(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int stride);

    public enum ggml_op_pool {
        GGML_OP_POOL_MAX(0),
        GGML_OP_POOL_AVG(1),
        GGML_OP_POOL_COUNT(2);

        public final int value;
        private ggml_op_pool(int v) { this.value = v; }
        private ggml_op_pool(ggml_op_pool e) { this.value = e.value; }
        public ggml_op_pool intern() { for (ggml_op_pool e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }

    public static native ggml_tensor ggml_pool_1d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_op_pool op,
                int k0,
                int s0,
                int p0);
    public static native ggml_tensor ggml_pool_1d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("ggml_op_pool") int op,
                int k0,
                int s0,
                int p0); // padding

    // the result will have 2*p0 padding for the first dimension
    // and 2*p1 padding for the second dimension
    public static native ggml_tensor ggml_pool_2d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_op_pool op,
                int k0,
                int k1,
                int s0,
                int s1,
                float p0,
                float p1);
    public static native ggml_tensor ggml_pool_2d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("ggml_op_pool") int op,
                int k0,
                int k1,
                int s0,
                int s1,
                float p0,
                float p1);

    public static native ggml_tensor ggml_pool_2d_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor af,
                ggml_op_pool op,
                int k0,
                int k1,
                int s0,
                int s1,
                float p0,
                float p1);
    public static native ggml_tensor ggml_pool_2d_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor af,
                @Cast("ggml_op_pool") int op,
                int k0,
                int k1,
                int s0,
                int s1,
                float p0,
                float p1);

    // nearest interpolate
    // multiplies ne0 and ne1 by scale factor
    // used in stable-diffusion
    public static native ggml_tensor ggml_upscale(
                ggml_context ctx,
                ggml_tensor a,
                int scale_factor);

    // nearest interpolate
    // nearest interpolate to specified dimensions
    // used in tortoise.cpp
    public static native ggml_tensor ggml_upscale_ext(
                ggml_context ctx,
                ggml_tensor a,
                int ne0,
                int ne1,
                int ne2,
                int ne3);

    // pad each dimension with zeros: [x, ..., x] -> [x, ..., x, 0, ..., 0]
    public static native ggml_tensor ggml_pad(
                ggml_context ctx,
                ggml_tensor a,
                int p0,
                int p1,
                int p2,
                int p3);

    // pad each dimension with reflection: [a, b, c, d] -> [b, a, b, c, d, c]
    public static native ggml_tensor ggml_pad_reflect_1d(
                ggml_context ctx,
                ggml_tensor a,
                int p0,
                int p1);

    // Ref: https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/util.py#L151
    // timesteps: [N,]
    // return: [N, dim]
    public static native ggml_tensor ggml_timestep_embedding(
                ggml_context ctx,
                ggml_tensor timesteps,
                int dim,
                int max_period);

    // sort rows
    public enum ggml_sort_order {
        GGML_SORT_ORDER_ASC(0),
        GGML_SORT_ORDER_DESC(1);

        public final int value;
        private ggml_sort_order(int v) { this.value = v; }
        private ggml_sort_order(ggml_sort_order e) { this.value = e.value; }
        public ggml_sort_order intern() { for (ggml_sort_order e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }

    public static native ggml_tensor ggml_argsort(
                ggml_context ctx,
                ggml_tensor a,
                ggml_sort_order order);
    public static native ggml_tensor ggml_argsort(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("ggml_sort_order") int order);

    public static native ggml_tensor ggml_arange(
                ggml_context ctx,
                float start,
                float stop,
                float step);

    // top k elements per row
    public static native ggml_tensor ggml_top_k(
                ggml_context ctx,
                ggml_tensor a,
                int k);

public static final int GGML_KQ_MASK_PAD = 64;

    // q:    [n_embd, n_batch,     n_head,    1]
    // k:    [n_embd, n_kv,        n_head_kv, 1]
    // v:    [n_embd, n_kv,        n_head_kv, 1] !! not transposed !!
    // mask: [n_kv,   n_batch_pad, 1,         1] !! n_batch_pad = GGML_PAD(n_batch, GGML_KQ_MASK_PAD) !!
    // res:  [n_embd, n_head,      n_batch,   1] !! permuted !!
    public static native ggml_tensor ggml_flash_attn_ext(
                ggml_context ctx,
                ggml_tensor q,
                ggml_tensor k,
                ggml_tensor v,
                ggml_tensor mask,
                float scale,
                float max_bias,
                float logit_softcap);

    public static native void ggml_flash_attn_ext_set_prec(
                ggml_tensor a,
                ggml_prec prec);
    public static native void ggml_flash_attn_ext_set_prec(
                ggml_tensor a,
                @Cast("ggml_prec") int prec);

    public static native ggml_prec ggml_flash_attn_ext_get_prec(
                @Const ggml_tensor a);

    // TODO: needs to be adapted to ggml_flash_attn_ext
    public static native ggml_tensor ggml_flash_attn_back(
               ggml_context ctx,
               ggml_tensor q,
               ggml_tensor k,
               ggml_tensor v,
               ggml_tensor d,
               @Cast("bool") boolean masked);

    public static native ggml_tensor ggml_ssm_conv(
                ggml_context ctx,
                ggml_tensor sx,
                ggml_tensor c);

    public static native ggml_tensor ggml_ssm_scan(
                ggml_context ctx,
                ggml_tensor s,
                ggml_tensor x,
                ggml_tensor dt,
                ggml_tensor A,
                ggml_tensor B,
                ggml_tensor C);

    // partition into non-overlapping windows with padding if needed
    // example:
    // a:   768   64   64    1
    // w:    14
    // res: 768   14   14    25
    // used in sam
    public static native ggml_tensor ggml_win_part(
                ggml_context ctx,
                ggml_tensor a,
                int w);

    // reverse of ggml_win_part
    // used in sam
    public static native ggml_tensor ggml_win_unpart(
                ggml_context ctx,
                ggml_tensor a,
                int w0,
                int h0,
                int w);

    public static native ggml_tensor ggml_unary(
                ggml_context ctx,
                 ggml_tensor a,
                 ggml_unary_op op);
    public static native ggml_tensor ggml_unary(
                ggml_context ctx,
                 ggml_tensor a,
                 @Cast("ggml_unary_op") int op);

    public static native ggml_tensor ggml_unary_inplace(
            ggml_context ctx,
            ggml_tensor a,
            ggml_unary_op op);
    public static native ggml_tensor ggml_unary_inplace(
            ggml_context ctx,
            ggml_tensor a,
            @Cast("ggml_unary_op") int op);

    // used in sam
    public static native ggml_tensor ggml_get_rel_pos(
                ggml_context ctx,
                ggml_tensor a,
                int qh,
                int kh);

    // used in sam
    public static native ggml_tensor ggml_add_rel_pos(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor pw,
                ggml_tensor ph);

    public static native ggml_tensor ggml_add_rel_pos_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor pw,
                ggml_tensor ph);

    public static native ggml_tensor ggml_rwkv_wkv6(
                ggml_context ctx,
                ggml_tensor k,
                ggml_tensor v,
                ggml_tensor r,
                ggml_tensor tf,
                ggml_tensor td,
                ggml_tensor state);

    public static native ggml_tensor ggml_gated_linear_attn(
                ggml_context ctx,
                ggml_tensor k,
                ggml_tensor v,
                ggml_tensor q,
                ggml_tensor g,
                ggml_tensor state,
                float scale);
// Targeting ../ggml_unary_op_f32_t.java


// Targeting ../ggml_binary_op_f32_t.java


// Targeting ../ggml_custom1_op_f32_t.java


// Targeting ../ggml_custom2_op_f32_t.java


// Targeting ../ggml_custom3_op_f32_t.java



    public static native ggml_tensor ggml_map_unary_f32(
                ggml_context ctx,
                ggml_tensor a,
                       ggml_unary_op_f32_t fun);

    public static native ggml_tensor ggml_map_unary_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                       ggml_unary_op_f32_t fun);

    public static native ggml_tensor ggml_map_binary_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                       ggml_binary_op_f32_t fun);

    public static native ggml_tensor ggml_map_binary_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                       ggml_binary_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom1_f32(
                ggml_context ctx,
                ggml_tensor a,
                       ggml_custom1_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom1_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                       ggml_custom1_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom2_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                       ggml_custom2_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom2_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                       ggml_custom2_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom3_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                       ggml_custom3_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom3_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                       ggml_custom3_op_f32_t fun);
// Targeting ../ggml_custom1_op_t.java


// Targeting ../ggml_custom2_op_t.java


// Targeting ../ggml_custom3_op_t.java



public static final int GGML_N_TASKS_MAX = (-1);
    // n_tasks == GGML_N_TASKS_MAX means to use max number of tasks

    public static native ggml_tensor ggml_map_custom1(
                ggml_context ctx,
                ggml_tensor a,
                ggml_custom1_op_t fun,
                int n_tasks,
                Pointer userdata);

    public static native ggml_tensor ggml_map_custom1_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_custom1_op_t fun,
                int n_tasks,
                Pointer userdata);

    public static native ggml_tensor ggml_map_custom2(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_custom2_op_t fun,
                int n_tasks,
                Pointer userdata);

    public static native ggml_tensor ggml_map_custom2_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_custom2_op_t fun,
                int n_tasks,
                Pointer userdata);

    public static native ggml_tensor ggml_map_custom3(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                ggml_custom3_op_t fun,
                int n_tasks,
                Pointer userdata);

    public static native ggml_tensor ggml_map_custom3_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                ggml_custom3_op_t fun,
                int n_tasks,
                Pointer userdata);

    // loss function

    public static native ggml_tensor ggml_cross_entropy_loss(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b); // labels

    public static native ggml_tensor ggml_cross_entropy_loss_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c); // gradients of cross_entropy_loss result

    // AdamW optimizer step
    // Paper: https://arxiv.org/pdf/1711.05101v3.pdf
    // PyTorch: https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html
    public static native ggml_tensor ggml_opt_step_adamw(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor grad,
                ggml_tensor m,
                ggml_tensor v,
                ggml_tensor adamw_params); // parameters such a the learning rate

    //
    // automatic differentiation
    //

    public static native void ggml_build_forward_expand(ggml_cgraph cgraph, ggml_tensor tensor);
    public static native void ggml_build_backward_expand(
            ggml_context ctx_static,
            ggml_context ctx_compute,
            ggml_cgraph cgraph,
            @Cast("bool") boolean accumulate); // whether or not gradients should be accumulated, requires static allocation of tensors in ctx_static

    // graph allocation in a context
    public static native ggml_cgraph ggml_new_graph(ggml_context ctx); // size = GGML_DEFAULT_GRAPH_SIZE, grads = false
    public static native ggml_cgraph ggml_new_graph_custom(ggml_context ctx, @Cast("size_t") long size, @Cast("bool") boolean grads);
    public static native ggml_cgraph ggml_graph_dup(ggml_context ctx, ggml_cgraph cgraph);
    public static native void ggml_graph_cpy(ggml_cgraph src, ggml_cgraph dst);
    public static native void ggml_graph_reset(ggml_cgraph cgraph); // set regular grads + optimizer momenta to 0, set loss grad to 1
    public static native void ggml_graph_clear(ggml_cgraph cgraph);

    public static native int ggml_graph_size(ggml_cgraph cgraph);
    public static native ggml_tensor ggml_graph_node(ggml_cgraph cgraph, int i); // if i < 0, returns nodes[n_nodes + i]
    public static native @Cast("ggml_tensor**") PointerPointer ggml_graph_nodes(ggml_cgraph cgraph);
    public static native int ggml_graph_n_nodes(ggml_cgraph cgraph);

    public static native void ggml_graph_add_node(ggml_cgraph cgraph, ggml_tensor tensor);

    public static native @Cast("size_t") long ggml_graph_overhead();
    public static native @Cast("size_t") long ggml_graph_overhead_custom(@Cast("size_t") long size, @Cast("bool") boolean grads);

    public static native ggml_tensor ggml_graph_get_tensor(@Const ggml_cgraph cgraph, @Cast("const char*") BytePointer name);
    public static native ggml_tensor ggml_graph_get_tensor(@Const ggml_cgraph cgraph, String name);
    public static native ggml_tensor ggml_graph_get_grad(@Const ggml_cgraph cgraph, @Const ggml_tensor node);
    public static native ggml_tensor ggml_graph_get_grad_acc(@Const ggml_cgraph cgraph, @Const ggml_tensor node);

    
    

    // print info and performance information for the graph
    public static native void ggml_graph_print(@Const ggml_cgraph cgraph);

    // dump the graph into a file using the dot format
    public static native void ggml_graph_dump_dot(@Const ggml_cgraph gb, @Const ggml_cgraph gf, @Cast("const char*") BytePointer filename);
    public static native void ggml_graph_dump_dot(@Const ggml_cgraph gb, @Const ggml_cgraph gf, String filename);
// Targeting ../ggml_log_callback.java



    // Set callback for all future logging events.
    // If this is not called, or NULL is supplied, everything is output on stderr.
    public static native void ggml_log_set(ggml_log_callback log_callback, Pointer user_data);

    public static native ggml_tensor ggml_set_zero(ggml_tensor tensor);

    //
    // quantization
    //

    // - ggml_quantize_init can be called multiple times with the same type
    //   it will only initialize the quantization tables for the first call or after ggml_quantize_free
    //   automatically called by ggml_quantize_chunk for convenience
    //
    // - ggml_quantize_free will free any memory allocated by ggml_quantize_init
    //   call this at the end of the program to avoid memory leaks
    //
    // note: these are thread-safe
    //
    public static native void ggml_quantize_init(ggml_type type);
    public static native void ggml_quantize_init(@Cast("ggml_type") int type);
    public static native void ggml_quantize_free();

    // some quantization type cannot be used without an importance matrix
    public static native @Cast("bool") boolean ggml_quantize_requires_imatrix(ggml_type type);
    public static native @Cast("bool") boolean ggml_quantize_requires_imatrix(@Cast("ggml_type") int type);

    // calls ggml_quantize_init internally (i.e. can allocate memory)
    public static native @Cast("size_t") long ggml_quantize_chunk(
                ggml_type type,
                   @Const FloatPointer src,
                          Pointer dst,
                       @Cast("int64_t") long start,
                       @Cast("int64_t") long nrows,
                       @Cast("int64_t") long n_per_row,
                   @Const FloatPointer imatrix);
    public static native @Cast("size_t") long ggml_quantize_chunk(
                @Cast("ggml_type") int type,
                   @Const FloatBuffer src,
                          Pointer dst,
                       @Cast("int64_t") long start,
                       @Cast("int64_t") long nrows,
                       @Cast("int64_t") long n_per_row,
                   @Const FloatBuffer imatrix);
    public static native @Cast("size_t") long ggml_quantize_chunk(
                ggml_type type,
                   @Const float[] src,
                          Pointer dst,
                       @Cast("int64_t") long start,
                       @Cast("int64_t") long nrows,
                       @Cast("int64_t") long n_per_row,
                   @Const float[] imatrix);
    public static native @Cast("size_t") long ggml_quantize_chunk(
                @Cast("ggml_type") int type,
                   @Const FloatPointer src,
                          Pointer dst,
                       @Cast("int64_t") long start,
                       @Cast("int64_t") long nrows,
                       @Cast("int64_t") long n_per_row,
                   @Const FloatPointer imatrix);
    public static native @Cast("size_t") long ggml_quantize_chunk(
                ggml_type type,
                   @Const FloatBuffer src,
                          Pointer dst,
                       @Cast("int64_t") long start,
                       @Cast("int64_t") long nrows,
                       @Cast("int64_t") long n_per_row,
                   @Const FloatBuffer imatrix);
    public static native @Cast("size_t") long ggml_quantize_chunk(
                @Cast("ggml_type") int type,
                   @Const float[] src,
                          Pointer dst,
                       @Cast("int64_t") long start,
                       @Cast("int64_t") long nrows,
                       @Cast("int64_t") long n_per_row,
                   @Const float[] imatrix);

// #ifdef __cplusplus
    // restrict not standard in C++
// #    if defined(__GNUC__)
// #        define GGML_RESTRICT __restrict__
// #    elif defined(__clang__)
// #        define GGML_RESTRICT __restrict
// #    elif defined(_MSC_VER)
// #        define GGML_RESTRICT __restrict
// #    else
// #        define GGML_RESTRICT
// #    endif
// #else
// #    if defined (_MSC_VER) && (__STDC_VERSION__ < 201112L)
// #        define GGML_RESTRICT __restrict
// #    else
// #        define GGML_RESTRICT restrict
// #    endif
// #endif
// Targeting ../ggml_type_traits.java



    public static native @Const ggml_type_traits ggml_get_type_traits(ggml_type type);
    public static native @Const ggml_type_traits ggml_get_type_traits(@Cast("ggml_type") int type);

    // ggml threadpool
    // TODO: currently, only a few functions are in the base ggml API, while the rest are in the CPU backend
    // the goal should be to create an API that other backends can use move everything to the ggml base

    // scheduling priorities
    public enum ggml_sched_priority {
        GGML_SCHED_PRIO_NORMAL(0),
        GGML_SCHED_PRIO_MEDIUM(1),
        GGML_SCHED_PRIO_HIGH(2),
        GGML_SCHED_PRIO_REALTIME(3);

        public final int value;
        private ggml_sched_priority(int v) { this.value = v; }
        private ggml_sched_priority(ggml_sched_priority e) { this.value = e.value; }
        public ggml_sched_priority intern() { for (ggml_sched_priority e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }
// Targeting ../ggml_threadpool_params.java


// Targeting ../ggml_threadpool.java

     // forward declaration, see ggml.c

    public static native @ByVal ggml_threadpool_params ggml_threadpool_params_default(int n_threads);
    public static native void ggml_threadpool_params_init(ggml_threadpool_params p, int n_threads);
    public static native @Cast("bool") boolean ggml_threadpool_params_match(@Const ggml_threadpool_params p0, @Const ggml_threadpool_params p1);

// #ifdef  __cplusplus
// #endif


// Parsed from ggml-alloc.h

// #pragma once

// #include "ggml.h"

// #ifdef  __cplusplus
// #endif
// Targeting ../ggml_tallocr.java



public static native @ByVal ggml_tallocr ggml_tallocr_new(ggml_backend_buffer buffer);
public static native ggml_status ggml_tallocr_alloc(ggml_tallocr talloc, ggml_tensor tensor);
// Targeting ../ggml_gallocr.java


public static native void ggml_gallocr_free(ggml_gallocr galloc);

// pre-allocate buffers from a measure graph - does not allocate or modify the graph
// call with a worst-case graph to avoid buffer reallocations
// not strictly required for single buffer usage: ggml_gallocr_alloc_graph will reallocate the buffers automatically if needed
// returns false if the buffer allocation failed
public static native @Cast("bool") boolean ggml_gallocr_reserve(ggml_gallocr galloc, ggml_cgraph graph);
public static native @Cast("bool") boolean ggml_gallocr_reserve_n(
    ggml_gallocr galloc,
    ggml_cgraph graph,
    @Const IntPointer node_buffer_ids,
    @Const IntPointer leaf_buffer_ids);
public static native @Cast("bool") boolean ggml_gallocr_reserve_n(
    ggml_gallocr galloc,
    ggml_cgraph graph,
    @Const IntBuffer node_buffer_ids,
    @Const IntBuffer leaf_buffer_ids);
public static native @Cast("bool") boolean ggml_gallocr_reserve_n(
    ggml_gallocr galloc,
    ggml_cgraph graph,
    @Const int[] node_buffer_ids,
    @Const int[] leaf_buffer_ids);

// automatic reallocation if the topology changes when using a single buffer
// returns false if using multiple buffers and a re-allocation is needed (call ggml_gallocr_reserve_n first to set the node buffers)
public static native @Cast("bool") boolean ggml_gallocr_alloc_graph(ggml_gallocr galloc, ggml_cgraph graph);

public static native @Cast("size_t") long ggml_gallocr_get_buffer_size(ggml_gallocr galloc, int buffer_id);

// Utils
// Create a buffer and allocate all the tensors in a ggml_context
public static native ggml_backend_buffer ggml_backend_alloc_ctx_tensors(ggml_context ctx, ggml_backend backend);

// #ifdef  __cplusplus
// #endif


// Parsed from ggml-cpu.h

// #pragma once

// #include "ggml.h"
// #include "ggml-backend.h"

// #ifdef  __cplusplus
// Targeting ../ggml_cplan.java



    // numa strategies
    public enum ggml_numa_strategy {
        GGML_NUMA_STRATEGY_DISABLED  (0),
        GGML_NUMA_STRATEGY_DISTRIBUTE(1),
        GGML_NUMA_STRATEGY_ISOLATE   (2),
        GGML_NUMA_STRATEGY_NUMACTL   (3),
        GGML_NUMA_STRATEGY_MIRROR    (4),
        GGML_NUMA_STRATEGY_COUNT(5);

        public final int value;
        private ggml_numa_strategy(int v) { this.value = v; }
        private ggml_numa_strategy(ggml_numa_strategy e) { this.value = e.value; }
        public ggml_numa_strategy intern() { for (ggml_numa_strategy e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    } // call once for better performance on NUMA systems // true if init detected that system has >1 NUMA node

    // ggml_graph_plan() has to be called before ggml_graph_compute()
    // when plan.work_size > 0, caller must allocate memory for plan.work_data

    // same as ggml_graph_compute() but the work data is allocated as a part of the context
    // note: the drawback of this API is that you must have ensured that the context has enough memory for the work data

    //
    // system info
    //

    // x86
    // ARM  // sve vector length in bytes
    // other

    // Internal types and functions exposed for tests and benchmarks
// Targeting ../ggml_type_traits_cpu.java



    //
    // CPU backend
    //

// #ifdef __cplusplus
// #endif


// Parsed from llama.h

// #ifndef LLAMA_H
// #define LLAMA_H

// #include "ggml.h"
// #include "ggml-cpu.h"
// #include "ggml-backend.h"

// #include <stddef.h>
// #include <stdint.h>
// #include <stdio.h>
// #include <stdbool.h>

// #ifdef LLAMA_SHARED
// #    if defined(_WIN32) && !defined(__MINGW32__)
// #        ifdef LLAMA_BUILD
// #            define LLAMA_API __declspec(dllexport)
// #        else
// #            define LLAMA_API __declspec(dllimport)
// #        endif
// #    else
// #        define LLAMA_API __attribute__ ((visibility ("default")))
// #    endif
// #else
// #    define LLAMA_API
// #endif

// #ifdef __GNUC__
// #    define DEPRECATED(func, hint) func __attribute__((deprecated(hint)))
// #elif defined(_MSC_VER)
// #    define DEPRECATED(func, hint) __declspec(deprecated(hint)) func
// #else
// #    define DEPRECATED(func, hint) func
// #endif

public static final int LLAMA_DEFAULT_SEED = 0xFFFFFFFF;

public static final int LLAMA_TOKEN_NULL = -1;

public static final int LLAMA_FILE_MAGIC_GGLA = 0x67676c61; // 'ggla'
public static final int LLAMA_FILE_MAGIC_GGSN = 0x6767736e; // 'ggsn'
public static final int LLAMA_FILE_MAGIC_GGSQ = 0x67677371; // 'ggsq'

public static final int LLAMA_SESSION_MAGIC =   LLAMA_FILE_MAGIC_GGSN;
public static final int LLAMA_SESSION_VERSION = 9;

public static final int LLAMA_STATE_SEQ_MAGIC =   LLAMA_FILE_MAGIC_GGSQ;
public static final int LLAMA_STATE_SEQ_VERSION = 2;

// #ifdef __cplusplus
// Targeting ../llama_vocab.java


// Targeting ../llama_model.java


// Targeting ../llama_context.java



    

    // pre-tokenization types
    public enum llama_vocab_pre_type {
        LLAMA_VOCAB_PRE_TYPE_DEFAULT       (0),
        LLAMA_VOCAB_PRE_TYPE_LLAMA3        (1),
        LLAMA_VOCAB_PRE_TYPE_DEEPSEEK_LLM  (2),
        LLAMA_VOCAB_PRE_TYPE_DEEPSEEK_CODER(3),
        LLAMA_VOCAB_PRE_TYPE_FALCON        (4),
        LLAMA_VOCAB_PRE_TYPE_MPT           (5),
        LLAMA_VOCAB_PRE_TYPE_STARCODER     (6),
        LLAMA_VOCAB_PRE_TYPE_GPT2          (7),
        LLAMA_VOCAB_PRE_TYPE_REFACT        (8),
        LLAMA_VOCAB_PRE_TYPE_COMMAND_R     (9),
        LLAMA_VOCAB_PRE_TYPE_STABLELM2     (10),
        LLAMA_VOCAB_PRE_TYPE_QWEN2         (11),
        LLAMA_VOCAB_PRE_TYPE_OLMO          (12),
        LLAMA_VOCAB_PRE_TYPE_DBRX          (13),
        LLAMA_VOCAB_PRE_TYPE_SMAUG         (14),
        LLAMA_VOCAB_PRE_TYPE_PORO          (15),
        LLAMA_VOCAB_PRE_TYPE_CHATGLM3      (16),
        LLAMA_VOCAB_PRE_TYPE_CHATGLM4      (17),
        LLAMA_VOCAB_PRE_TYPE_VIKING        (18),
        LLAMA_VOCAB_PRE_TYPE_JAIS          (19),
        LLAMA_VOCAB_PRE_TYPE_TEKKEN        (20),
        LLAMA_VOCAB_PRE_TYPE_SMOLLM        (21),
        LLAMA_VOCAB_PRE_TYPE_CODESHELL     (22),
        LLAMA_VOCAB_PRE_TYPE_BLOOM         (23),
        LLAMA_VOCAB_PRE_TYPE_GPT3_FINNISH  (24),
        LLAMA_VOCAB_PRE_TYPE_EXAONE        (25),
        LLAMA_VOCAB_PRE_TYPE_CHAMELEON     (26),
        LLAMA_VOCAB_PRE_TYPE_MINERVA       (27),
        LLAMA_VOCAB_PRE_TYPE_DEEPSEEK3_LLM (28),
        LLAMA_VOCAB_PRE_TYPE_GPT4O         (29);

        public final int value;
        private llama_vocab_pre_type(int v) { this.value = v; }
        private llama_vocab_pre_type(llama_vocab_pre_type e) { this.value = e.value; }
        public llama_vocab_pre_type intern() { for (llama_vocab_pre_type e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }

    public enum llama_rope_type {
        LLAMA_ROPE_TYPE_NONE  (-1),
        LLAMA_ROPE_TYPE_NORM  (0);

        public final int value;
        private llama_rope_type(int v) { this.value = v; }
        private llama_rope_type(llama_rope_type e) { this.value = e.value; }
        public llama_rope_type intern() { for (llama_rope_type e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }

    public enum llama_token_type { //TODO: remove, required until per token attributes are available from GGUF file
        LLAMA_TOKEN_TYPE_UNDEFINED   (0),
        LLAMA_TOKEN_TYPE_NORMAL      (1),
        LLAMA_TOKEN_TYPE_UNKNOWN     (2),
        LLAMA_TOKEN_TYPE_CONTROL     (3),
        LLAMA_TOKEN_TYPE_USER_DEFINED(4),
        LLAMA_TOKEN_TYPE_UNUSED      (5),
        LLAMA_TOKEN_TYPE_BYTE        (6);

        public final int value;
        private llama_token_type(int v) { this.value = v; }
        private llama_token_type(llama_token_type e) { this.value = e.value; }
        public llama_token_type intern() { for (llama_token_type e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }

    public enum llama_token_attr {
        LLAMA_TOKEN_ATTR_UNDEFINED   (0),
        LLAMA_TOKEN_ATTR_UNKNOWN     (1 << 0),
        LLAMA_TOKEN_ATTR_UNUSED      (1 << 1),
        LLAMA_TOKEN_ATTR_NORMAL      (1 << 2),
        LLAMA_TOKEN_ATTR_CONTROL     (1 << 3),  // SPECIAL?
        LLAMA_TOKEN_ATTR_USER_DEFINED(1 << 4),
        LLAMA_TOKEN_ATTR_BYTE        (1 << 5),
        LLAMA_TOKEN_ATTR_NORMALIZED  (1 << 6),
        LLAMA_TOKEN_ATTR_LSTRIP      (1 << 7),
        LLAMA_TOKEN_ATTR_RSTRIP      (1 << 8),
        LLAMA_TOKEN_ATTR_SINGLE_WORD (1 << 9);

        public final int value;
        private llama_token_attr(int v) { this.value = v; }
        private llama_token_attr(llama_token_attr e) { this.value = e.value; }
        public llama_token_attr intern() { for (llama_token_attr e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }

    // model file types
    public enum llama_ftype {
        LLAMA_FTYPE_ALL_F32             (0),
        LLAMA_FTYPE_MOSTLY_F16          (1),  // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q4_0         (2),  // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q4_1         (3),  // except 1d tensors
        // LLAMA_FTYPE_MOSTLY_Q4_1_SOME_F16 = 4,  // tok_embeddings.weight and output.weight are F16
        // LLAMA_FTYPE_MOSTLY_Q4_2       = 5,  // support has been removed
        // LLAMA_FTYPE_MOSTLY_Q4_3       = 6,  // support has been removed
        LLAMA_FTYPE_MOSTLY_Q8_0         (7),  // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q5_0         (8),  // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q5_1         (9),  // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q2_K         (10), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q3_K_S       (11), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q3_K_M       (12), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q3_K_L       (13), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q4_K_S       (14), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q4_K_M       (15), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q5_K_S       (16), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q5_K_M       (17), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q6_K         (18), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_IQ2_XXS      (19), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_IQ2_XS       (20), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q2_K_S       (21), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_IQ3_XS       (22), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_IQ3_XXS      (23), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_IQ1_S        (24), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_IQ4_NL       (25), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_IQ3_S        (26), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_IQ3_M        (27), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_IQ2_S        (28), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_IQ2_M        (29), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_IQ4_XS       (30), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_IQ1_M        (31), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_BF16         (32), // except 1d tensors
        //LLAMA_FTYPE_MOSTLY_Q4_0_4_4      = 33, // removed from gguf files, use Q4_0 and runtime repack
        //LLAMA_FTYPE_MOSTLY_Q4_0_4_8      = 34, // removed from gguf files, use Q4_0 and runtime repack
        //LLAMA_FTYPE_MOSTLY_Q4_0_8_8      = 35, // removed from gguf files, use Q4_0 and runtime repack
        LLAMA_FTYPE_MOSTLY_TQ1_0        (36), // except 1d tensors
        LLAMA_FTYPE_MOSTLY_TQ2_0        (37), // except 1d tensors

        LLAMA_FTYPE_GUESSED(1024);// not specified in the model file

        public final int value;
        private llama_ftype(int v) { this.value = v; }
        private llama_ftype(llama_ftype e) { this.value = e.value; }
        public llama_ftype intern() { for (llama_ftype e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }

    public enum llama_rope_scaling_type {
        LLAMA_ROPE_SCALING_TYPE_UNSPECIFIED(-1),
        LLAMA_ROPE_SCALING_TYPE_NONE       (0),
        LLAMA_ROPE_SCALING_TYPE_LINEAR     (1),
        LLAMA_ROPE_SCALING_TYPE_YARN       (2),
        LLAMA_ROPE_SCALING_TYPE_LONGROPE   (3),
        LLAMA_ROPE_SCALING_TYPE_MAX_VALUE  (LLAMA_ROPE_SCALING_TYPE_LONGROPE.value);

        public final int value;
        private llama_rope_scaling_type(int v) { this.value = v; }
        private llama_rope_scaling_type(llama_rope_scaling_type e) { this.value = e.value; }
        public llama_rope_scaling_type intern() { for (llama_rope_scaling_type e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }

    

    public enum llama_attention_type {
        LLAMA_ATTENTION_TYPE_UNSPECIFIED(-1),
        LLAMA_ATTENTION_TYPE_CAUSAL     (0),
        LLAMA_ATTENTION_TYPE_NON_CAUSAL (1);

        public final int value;
        private llama_attention_type(int v) { this.value = v; }
        private llama_attention_type(llama_attention_type e) { this.value = e.value; }
        public llama_attention_type intern() { for (llama_attention_type e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }

    public enum llama_split_mode {
        LLAMA_SPLIT_MODE_NONE (0), // single GPU
        LLAMA_SPLIT_MODE_LAYER(1), // split layers and KV across GPUs
        LLAMA_SPLIT_MODE_ROW  (2);// split layers and KV across GPUs, use tensor parallelism if supported

        public final int value;
        private llama_split_mode(int v) { this.value = v; }
        private llama_split_mode(llama_split_mode e) { this.value = e.value; }
        public llama_split_mode intern() { for (llama_split_mode e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }
// Targeting ../llama_token_data.java


// Targeting ../llama_token_data_array.java


// Targeting ../llama_progress_callback.java


// Targeting ../llama_batch.java



    public enum llama_model_kv_override_type {
        LLAMA_KV_OVERRIDE_TYPE_INT(0),
        LLAMA_KV_OVERRIDE_TYPE_FLOAT(1),
        LLAMA_KV_OVERRIDE_TYPE_BOOL(2),
        LLAMA_KV_OVERRIDE_TYPE_STR(3);

        public final int value;
        private llama_model_kv_override_type(int v) { this.value = v; }
        private llama_model_kv_override_type(llama_model_kv_override_type e) { this.value = e.value; }
        public llama_model_kv_override_type intern() { for (llama_model_kv_override_type e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }
// Targeting ../llama_model_kv_override.java


// Targeting ../llama_model_params.java


// Targeting ../llama_context_params.java


// Targeting ../llama_model_quantize_params.java


// Targeting ../llama_logit_bias.java


// Targeting ../llama_sampler_chain_params.java


// Targeting ../llama_chat_message.java


// Targeting ../llama_adapter_lora.java



    // Helpers for getting default parameters
    // TODO: update API to start accepting pointers to params structs (https://github.com/ggml-org/llama.cpp/discussions/9172)
    public static native @ByVal llama_model_params llama_model_default_params();
    public static native @ByVal llama_context_params llama_context_default_params();
    public static native @ByVal llama_sampler_chain_params llama_sampler_chain_default_params();
    public static native @ByVal llama_model_quantize_params llama_model_quantize_default_params();

    // Initialize the llama + ggml backend
    // If numa is true, use NUMA optimizations
    // Call once at the start of the program
    public static native void llama_backend_init();

    // Call once at the end of the program - currently only used for MPI
    public static native void llama_backend_free();

    //optional:
    

    // Optional: an auto threadpool gets created in ggml if not passed explicitly
    

    public static native void llama_detach_threadpool(llama_context ctx);

    public static native llama_model llama_load_model_from_file(
                                 @Cast("const char*") BytePointer path_model,
                  @ByVal llama_model_params params);
    public static native llama_model llama_load_model_from_file(
                                 String path_model,
                  @ByVal llama_model_params params);

    // Load the model from a file
    // If the file is split into multiple parts, the file name must follow this pattern: <name>-%05d-of-%05d.gguf
    // If the split file name does not follow this pattern, use llama_model_load_from_splits
    public static native llama_model llama_model_load_from_file(
                                 @Cast("const char*") BytePointer path_model,
                  @ByVal llama_model_params params);
    public static native llama_model llama_model_load_from_file(
                                 String path_model,
                  @ByVal llama_model_params params);

    // Load the model from multiple splits (support custom naming scheme)
    // The paths must be in the correct order
    public static native llama_model llama_model_load_from_splits(
                                 @Cast("const char**") PointerPointer paths,
                                     @Cast("size_t") long n_paths,
                  @ByVal llama_model_params params);
    public static native llama_model llama_model_load_from_splits(
                                 @Cast("const char**") @ByPtrPtr BytePointer paths,
                                     @Cast("size_t") long n_paths,
                  @ByVal llama_model_params params);
    public static native llama_model llama_model_load_from_splits(
                                 @Cast("const char**") @ByPtrPtr ByteBuffer paths,
                                     @Cast("size_t") long n_paths,
                  @ByVal llama_model_params params);
    public static native llama_model llama_model_load_from_splits(
                                 @Cast("const char**") @ByPtrPtr byte[] paths,
                                     @Cast("size_t") long n_paths,
                  @ByVal llama_model_params params);

    public static native void llama_free_model(llama_model model);

    public static native void llama_model_free(llama_model model);

    public static native llama_context llama_init_from_model(
                         llama_model model,
                @ByVal llama_context_params params);

    public static native llama_context llama_new_context_with_model(
                         llama_model model,
                @ByVal llama_context_params params);

    // Frees all allocated memory
    public static native void llama_free(llama_context ctx);

    public static native @Cast("int64_t") long llama_time_us();

    public static native @Cast("size_t") long llama_max_devices();

    public static native @Cast("bool") boolean llama_supports_mmap();
    public static native @Cast("bool") boolean llama_supports_mlock();
    public static native @Cast("bool") boolean llama_supports_gpu_offload();
    public static native @Cast("bool") boolean llama_supports_rpc();

    public static native @Cast("uint32_t") int llama_n_ctx(@Const llama_context ctx);
    public static native @Cast("uint32_t") int llama_n_batch(@Const llama_context ctx);
    public static native @Cast("uint32_t") int llama_n_ubatch(@Const llama_context ctx);
    public static native @Cast("uint32_t") int llama_n_seq_max(@Const llama_context ctx);

    public static native int llama_n_ctx_train(@Const llama_model model);
    public static native int llama_n_embd(@Const llama_model model);
    public static native int llama_n_layer(@Const llama_model model);
    public static native int llama_n_head(@Const llama_model model);

    public static native int llama_n_vocab(@Const llama_vocab vocab);

    public static native @Const llama_model llama_get_model(@Const llama_context ctx);
    

    public static native @Const llama_vocab llama_model_get_vocab(@Const llama_model model);
    public static native llama_rope_type llama_model_rope_type(@Const llama_model model);

    public static native int llama_model_n_ctx_train(@Const llama_model model);
    public static native int llama_model_n_embd(@Const llama_model model);
    public static native int llama_model_n_layer(@Const llama_model model);
    public static native int llama_model_n_head(@Const llama_model model);
    public static native int llama_model_n_head_kv(@Const llama_model model);

    // Get the model's RoPE frequency scaling factor
    public static native float llama_model_rope_freq_scale_train(@Const llama_model model);

    

    public static native int llama_vocab_n_tokens(@Const llama_vocab vocab);

    // Functions to access the model's GGUF metadata scalar values
    // - The functions return the length of the string on success, or -1 on failure
    // - The output string is always null-terminated and cleared on failure
    // - When retrieving a string, an extra byte must be allocated to account for the null terminator
    // - GGUF array values are not supported by these functions

    // Get metadata value as a string by key name
    public static native int llama_model_meta_val_str(@Const llama_model model, @Cast("const char*") BytePointer key, @Cast("char*") BytePointer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_meta_val_str(@Const llama_model model, String key, @Cast("char*") ByteBuffer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_meta_val_str(@Const llama_model model, @Cast("const char*") BytePointer key, @Cast("char*") byte[] buf, @Cast("size_t") long buf_size);
    public static native int llama_model_meta_val_str(@Const llama_model model, String key, @Cast("char*") BytePointer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_meta_val_str(@Const llama_model model, @Cast("const char*") BytePointer key, @Cast("char*") ByteBuffer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_meta_val_str(@Const llama_model model, String key, @Cast("char*") byte[] buf, @Cast("size_t") long buf_size);

    // Get the number of metadata key/value pairs
    public static native int llama_model_meta_count(@Const llama_model model);

    // Get metadata key name by index
    public static native int llama_model_meta_key_by_index(@Const llama_model model, int i, @Cast("char*") BytePointer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_meta_key_by_index(@Const llama_model model, int i, @Cast("char*") ByteBuffer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_meta_key_by_index(@Const llama_model model, int i, @Cast("char*") byte[] buf, @Cast("size_t") long buf_size);

    // Get metadata value as a string by index
    public static native int llama_model_meta_val_str_by_index(@Const llama_model model, int i, @Cast("char*") BytePointer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_meta_val_str_by_index(@Const llama_model model, int i, @Cast("char*") ByteBuffer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_meta_val_str_by_index(@Const llama_model model, int i, @Cast("char*") byte[] buf, @Cast("size_t") long buf_size);

    // Get a string describing the model type
    public static native int llama_model_desc(@Const llama_model model, @Cast("char*") BytePointer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_desc(@Const llama_model model, @Cast("char*") ByteBuffer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_desc(@Const llama_model model, @Cast("char*") byte[] buf, @Cast("size_t") long buf_size);

    // Returns the total size of all the tensors in the model in bytes
    public static native @Cast("uint64_t") long llama_model_size(@Const llama_model model);

    // Get the default chat template. Returns nullptr if not available
    // If name is NULL, returns the default chat template
    public static native @Cast("const char*") BytePointer llama_model_chat_template(@Const llama_model model, @Cast("const char*") BytePointer name);
    public static native String llama_model_chat_template(@Const llama_model model, String name);

    // Returns the total number of parameters in the model
    public static native @Cast("uint64_t") long llama_model_n_params(@Const llama_model model);

    // Returns true if the model contains an encoder that requires llama_encode() call
    public static native @Cast("bool") boolean llama_model_has_encoder(@Const llama_model model);

    // Returns true if the model contains a decoder that requires llama_decode() call
    public static native @Cast("bool") boolean llama_model_has_decoder(@Const llama_model model);

    // For encoder-decoder models, this function returns id of the token that must be provided
    // to the decoder to start generating output sequence. For other models, it returns -1.
    public static native @Cast("llama_token") int llama_model_decoder_start_token(@Const llama_model model);

    // Returns true if the model is recurrent (like Mamba, RWKV, etc.)
    public static native @Cast("bool") boolean llama_model_is_recurrent(@Const llama_model model);

    // Returns 0 on success
    public static native @Cast("uint32_t") int llama_model_quantize(
                @Cast("const char*") BytePointer fname_inp,
                @Cast("const char*") BytePointer fname_out,
                @Const llama_model_quantize_params params);
    public static native @Cast("uint32_t") int llama_model_quantize(
                String fname_inp,
                String fname_out,
                @Const llama_model_quantize_params params);

    //
    // Adapters
    //

    // Load a LoRA adapter from file
    public static native llama_adapter_lora llama_adapter_lora_init(
                llama_model model,
                @Cast("const char*") BytePointer path_lora);
    public static native llama_adapter_lora llama_adapter_lora_init(
                llama_model model,
                String path_lora);

    // Manually free a LoRA adapter
    // Note: loaded adapters will be free when the associated model is deleted
    public static native void llama_adapter_lora_free(llama_adapter_lora adapter);

    // The following functions operate on a llama_context, hence the naming: llama_verb_...

    // Add a loaded LoRA adapter to given context
    // This will not modify model's weight
    public static native int llama_set_adapter_lora(
                llama_context ctx,
                llama_adapter_lora adapter,
                float scale);

    // Remove a specific LoRA adapter from given context
    // Return -1 if the adapter is not present in the context
    public static native int llama_rm_adapter_lora(
                llama_context ctx,
                llama_adapter_lora adapter);

    // Remove all LoRA adapters from given context
    public static native void llama_clear_adapter_lora(llama_context ctx);

    // Apply a loaded control vector to a llama_context, or if data is NULL, clear
    // the currently loaded vector.
    // n_embd should be the size of a single layer's control, and data should point
    // to an n_embd x n_layers buffer starting from layer 1.
    // il_start and il_end are the layer range the vector should apply to (both inclusive)
    // See llama_control_vector_load in common to load a control vector.
    public static native int llama_apply_adapter_cvec(
                llama_context ctx,
                         @Const FloatPointer data,
                              @Cast("size_t") long len,
                             int n_embd,
                             int il_start,
                             int il_end);
    public static native int llama_apply_adapter_cvec(
                llama_context ctx,
                         @Const FloatBuffer data,
                              @Cast("size_t") long len,
                             int n_embd,
                             int il_start,
                             int il_end);
    public static native int llama_apply_adapter_cvec(
                llama_context ctx,
                         @Const float[] data,
                              @Cast("size_t") long len,
                             int n_embd,
                             int il_start,
                             int il_end);
// Targeting ../llama_kv_cache_view_cell.java


// Targeting ../llama_kv_cache_view.java



    // Create an empty KV cache view. (use only for debugging purposes)
    public static native @ByVal llama_kv_cache_view llama_kv_cache_view_init(@Const llama_context ctx, int n_seq_max);

    // Free a KV cache view. (use only for debugging purposes)
    public static native void llama_kv_cache_view_free(llama_kv_cache_view view);

    // Update the KV cache view structure with the current state of the KV cache. (use only for debugging purposes)
    // TODO: change signature to llama_kv_cache_view_update(struct llama_kv_cache_view * view, const struct llama_context * ctx)
    
    
    ///
    public static native void llama_kv_cache_view_update(@Const llama_context ctx, llama_kv_cache_view view);

    /** */

    // Returns the number of tokens in the KV cache (slow, use only for debug)
    // If a KV cell has multiple sequences assigned to it, it will be counted multiple times
    public static native int llama_get_kv_cache_token_count(@Const llama_context ctx);

    // Returns the number of used KV cells (i.e. have at least one sequence assigned to them)
    public static native int llama_get_kv_cache_used_cells(@Const llama_context ctx);

    // Clear the KV cache - both cell info is erased and KV data is zeroed
    public static native void llama_kv_cache_clear(
                llama_context ctx);

    // Removes all tokens that belong to the specified sequence and have positions in [p0, p1)
    // Returns false if a partial sequence cannot be removed. Removing a whole sequence never fails
    // seq_id < 0 : match any sequence
    // p0 < 0     : [0,  p1]
    // p1 < 0     : [p0, inf)
    public static native @Cast("bool") boolean llama_kv_cache_seq_rm(
                llama_context ctx,
                        @Cast("llama_seq_id") int seq_id,
                           @Cast("llama_pos") int p0,
                           @Cast("llama_pos") int p1);

    // Copy all tokens that belong to the specified sequence to another sequence
    // Note that this does not allocate extra KV cache memory - it simply assigns the tokens to the new sequence
    // p0 < 0 : [0,  p1]
    // p1 < 0 : [p0, inf)
    public static native void llama_kv_cache_seq_cp(
                llama_context ctx,
                        @Cast("llama_seq_id") int seq_id_src,
                        @Cast("llama_seq_id") int seq_id_dst,
                           @Cast("llama_pos") int p0,
                           @Cast("llama_pos") int p1);

    // Removes all tokens that do not belong to the specified sequence
    public static native void llama_kv_cache_seq_keep(
                llama_context ctx,
                        @Cast("llama_seq_id") int seq_id);

    // Adds relative position "delta" to all tokens that belong to the specified sequence and have positions in [p0, p1)
    // If the KV cache is RoPEd, the KV data is updated accordingly:
    //   - lazily on next llama_decode()
    //   - explicitly with llama_kv_cache_update()
    // p0 < 0 : [0,  p1]
    // p1 < 0 : [p0, inf)
    public static native void llama_kv_cache_seq_add(
                llama_context ctx,
                        @Cast("llama_seq_id") int seq_id,
                           @Cast("llama_pos") int p0,
                           @Cast("llama_pos") int p1,
                           @Cast("llama_pos") int delta);

    // Integer division of the positions by factor of `d > 1`
    // If the KV cache is RoPEd, the KV data is updated accordingly:
    //   - lazily on next llama_decode()
    //   - explicitly with llama_kv_cache_update()
    // p0 < 0 : [0,  p1]
    // p1 < 0 : [p0, inf)
    public static native void llama_kv_cache_seq_div(
                llama_context ctx,
                        @Cast("llama_seq_id") int seq_id,
                           @Cast("llama_pos") int p0,
                           @Cast("llama_pos") int p1,
                                 int d);

    // Returns the largest position present in the KV cache for the specified sequence
    public static native @Cast("llama_pos") int llama_kv_cache_seq_pos_max(
                llama_context ctx,
                        @Cast("llama_seq_id") int seq_id);

    // TODO: the llama_kv_cache_defrag and llama_kv_cache_update API tightly couples llama_context with llama_kv_cache
    //       how to avoid this?

    // Defragment the KV cache
    // This will be applied:
    //   - lazily on next llama_decode()
    //   - explicitly with llama_kv_cache_update()
    public static native void llama_kv_cache_defrag(llama_context ctx);

    // Apply the KV cache updates (such as K-shifts, defragmentation, etc.)
    public static native void llama_kv_cache_update(llama_context ctx);

    // Check if the context supports KV cache shifting
    public static native @Cast("bool") boolean llama_kv_cache_can_shift(llama_context ctx);

    //
    // State / sessions
    //

    // Returns the *actual* size in bytes of the state
    // (logits, embedding and kv_cache)
    // Only use when saving the state, not when restoring it, otherwise the size may be too small.
    public static native @Cast("size_t") long llama_state_get_size(llama_context ctx);
    public static native @Cast("size_t") long llama_get_state_size(llama_context ctx);

    // Copies the state to the specified destination address.
    // Destination needs to have allocated enough memory.
    // Returns the number of bytes copied
    public static native @Cast("size_t") long llama_state_get_data(
                llama_context ctx,
                             @Cast("uint8_t*") BytePointer dst,
                              @Cast("size_t") long size);
    public static native @Cast("size_t") long llama_state_get_data(
                llama_context ctx,
                             @Cast("uint8_t*") ByteBuffer dst,
                              @Cast("size_t") long size);
    public static native @Cast("size_t") long llama_state_get_data(
                llama_context ctx,
                             @Cast("uint8_t*") byte[] dst,
                              @Cast("size_t") long size);
    public static native @Cast("size_t") long llama_copy_state_data(
                llama_context ctx,
                             @Cast("uint8_t*") BytePointer dst);
    public static native @Cast("size_t") long llama_copy_state_data(
                llama_context ctx,
                             @Cast("uint8_t*") ByteBuffer dst);
    public static native @Cast("size_t") long llama_copy_state_data(
                llama_context ctx,
                             @Cast("uint8_t*") byte[] dst);

    // Set the state reading from the specified address
    // Returns the number of bytes read
    public static native @Cast("size_t") long llama_state_set_data(
                llama_context ctx,
                       @Cast("const uint8_t*") BytePointer src,
                              @Cast("size_t") long size);
    public static native @Cast("size_t") long llama_state_set_data(
                llama_context ctx,
                       @Cast("const uint8_t*") ByteBuffer src,
                              @Cast("size_t") long size);
    public static native @Cast("size_t") long llama_state_set_data(
                llama_context ctx,
                       @Cast("const uint8_t*") byte[] src,
                              @Cast("size_t") long size);
    public static native @Cast("size_t") long llama_set_state_data(
                llama_context ctx,
                       @Cast("const uint8_t*") BytePointer src);
    public static native @Cast("size_t") long llama_set_state_data(
                llama_context ctx,
                       @Cast("const uint8_t*") ByteBuffer src);
    public static native @Cast("size_t") long llama_set_state_data(
                llama_context ctx,
                       @Cast("const uint8_t*") byte[] src);

    // Save/load session file
    public static native @Cast("bool") boolean llama_state_load_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                         @Cast("llama_token*") IntPointer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_state_load_file(
                llama_context ctx,
                          String path_session,
                         @Cast("llama_token*") IntBuffer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_state_load_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                         @Cast("llama_token*") int[] tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_state_load_file(
                llama_context ctx,
                          String path_session,
                         @Cast("llama_token*") IntPointer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_state_load_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                         @Cast("llama_token*") IntBuffer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_state_load_file(
                llama_context ctx,
                          String path_session,
                         @Cast("llama_token*") int[] tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                         @Cast("llama_token*") IntPointer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(
                llama_context ctx,
                          String path_session,
                         @Cast("llama_token*") IntBuffer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                         @Cast("llama_token*") int[] tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(
                llama_context ctx,
                          String path_session,
                         @Cast("llama_token*") IntPointer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                         @Cast("llama_token*") IntBuffer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(
                llama_context ctx,
                          String path_session,
                         @Cast("llama_token*") int[] tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);

    public static native @Cast("bool") boolean llama_state_save_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                   @Cast("const llama_token*") IntPointer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_state_save_file(
                llama_context ctx,
                          String path_session,
                   @Cast("const llama_token*") IntBuffer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_state_save_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                   @Cast("const llama_token*") int[] tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_state_save_file(
                llama_context ctx,
                          String path_session,
                   @Cast("const llama_token*") IntPointer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_state_save_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                   @Cast("const llama_token*") IntBuffer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_state_save_file(
                llama_context ctx,
                          String path_session,
                   @Cast("const llama_token*") int[] tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                   @Cast("const llama_token*") IntPointer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(
                llama_context ctx,
                          String path_session,
                   @Cast("const llama_token*") IntBuffer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                   @Cast("const llama_token*") int[] tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(
                llama_context ctx,
                          String path_session,
                   @Cast("const llama_token*") IntPointer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer path_session,
                   @Cast("const llama_token*") IntBuffer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(
                llama_context ctx,
                          String path_session,
                   @Cast("const llama_token*") int[] tokens,
                              @Cast("size_t") long n_token_count);

    // Get the exact size needed to copy the KV cache of a single sequence
    public static native @Cast("size_t") long llama_state_seq_get_size(
                llama_context ctx,
                        @Cast("llama_seq_id") int seq_id);

    // Copy the KV cache of a single sequence into the specified buffer
    public static native @Cast("size_t") long llama_state_seq_get_data(
                llama_context ctx,
                             @Cast("uint8_t*") BytePointer dst,
                              @Cast("size_t") long size,
                        @Cast("llama_seq_id") int seq_id);
    public static native @Cast("size_t") long llama_state_seq_get_data(
                llama_context ctx,
                             @Cast("uint8_t*") ByteBuffer dst,
                              @Cast("size_t") long size,
                        @Cast("llama_seq_id") int seq_id);
    public static native @Cast("size_t") long llama_state_seq_get_data(
                llama_context ctx,
                             @Cast("uint8_t*") byte[] dst,
                              @Cast("size_t") long size,
                        @Cast("llama_seq_id") int seq_id);

    // Copy the sequence data (originally copied with `llama_state_seq_get_data`) into the specified sequence
    // Returns:
    //  - Positive: Ok
    //  - Zero: Failed to load
    public static native @Cast("size_t") long llama_state_seq_set_data(
                llama_context ctx,
                       @Cast("const uint8_t*") BytePointer src,
                              @Cast("size_t") long size,
                        @Cast("llama_seq_id") int dest_seq_id);
    public static native @Cast("size_t") long llama_state_seq_set_data(
                llama_context ctx,
                       @Cast("const uint8_t*") ByteBuffer src,
                              @Cast("size_t") long size,
                        @Cast("llama_seq_id") int dest_seq_id);
    public static native @Cast("size_t") long llama_state_seq_set_data(
                llama_context ctx,
                       @Cast("const uint8_t*") byte[] src,
                              @Cast("size_t") long size,
                        @Cast("llama_seq_id") int dest_seq_id);

    public static native @Cast("size_t") long llama_state_seq_save_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer filepath,
                        @Cast("llama_seq_id") int seq_id,
                   @Cast("const llama_token*") IntPointer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("size_t") long llama_state_seq_save_file(
                llama_context ctx,
                          String filepath,
                        @Cast("llama_seq_id") int seq_id,
                   @Cast("const llama_token*") IntBuffer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("size_t") long llama_state_seq_save_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer filepath,
                        @Cast("llama_seq_id") int seq_id,
                   @Cast("const llama_token*") int[] tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("size_t") long llama_state_seq_save_file(
                llama_context ctx,
                          String filepath,
                        @Cast("llama_seq_id") int seq_id,
                   @Cast("const llama_token*") IntPointer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("size_t") long llama_state_seq_save_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer filepath,
                        @Cast("llama_seq_id") int seq_id,
                   @Cast("const llama_token*") IntBuffer tokens,
                              @Cast("size_t") long n_token_count);
    public static native @Cast("size_t") long llama_state_seq_save_file(
                llama_context ctx,
                          String filepath,
                        @Cast("llama_seq_id") int seq_id,
                   @Cast("const llama_token*") int[] tokens,
                              @Cast("size_t") long n_token_count);

    public static native @Cast("size_t") long llama_state_seq_load_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer filepath,
                        @Cast("llama_seq_id") int dest_seq_id,
                         @Cast("llama_token*") IntPointer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("size_t") long llama_state_seq_load_file(
                llama_context ctx,
                          String filepath,
                        @Cast("llama_seq_id") int dest_seq_id,
                         @Cast("llama_token*") IntBuffer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("size_t") long llama_state_seq_load_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer filepath,
                        @Cast("llama_seq_id") int dest_seq_id,
                         @Cast("llama_token*") int[] tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("size_t") long llama_state_seq_load_file(
                llama_context ctx,
                          String filepath,
                        @Cast("llama_seq_id") int dest_seq_id,
                         @Cast("llama_token*") IntPointer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("size_t") long llama_state_seq_load_file(
                llama_context ctx,
                          @Cast("const char*") BytePointer filepath,
                        @Cast("llama_seq_id") int dest_seq_id,
                         @Cast("llama_token*") IntBuffer tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("size_t") long llama_state_seq_load_file(
                llama_context ctx,
                          String filepath,
                        @Cast("llama_seq_id") int dest_seq_id,
                         @Cast("llama_token*") int[] tokens_out,
                              @Cast("size_t") long n_token_capacity,
                              @Cast("size_t*") SizeTPointer n_token_count_out);

    //
    // Decoding
    //

    // Return batch for single sequence of tokens
    // The sequence ID will be fixed to 0
    // The position of the tokens will be tracked automatically by llama_decode
    //
    // NOTE: this is a helper function to facilitate transition to the new batch API - avoid using it
    //
    public static native @ByVal llama_batch llama_batch_get_one(
                      @Cast("llama_token*") IntPointer tokens,
                          int n_tokens);
    public static native @ByVal llama_batch llama_batch_get_one(
                      @Cast("llama_token*") IntBuffer tokens,
                          int n_tokens);
    public static native @ByVal llama_batch llama_batch_get_one(
                      @Cast("llama_token*") int[] tokens,
                          int n_tokens);

    // Allocates a batch of tokens on the heap that can hold a maximum of n_tokens
    // Each token can be assigned up to n_seq_max sequence ids
    // The batch has to be freed with llama_batch_free()
    // If embd != 0, llama_batch.embd will be allocated with size of n_tokens * embd * sizeof(float)
    // Otherwise, llama_batch.token will be allocated to store n_tokens llama_token
    // The rest of the llama_batch members are allocated with size n_tokens
    // All members are left uninitialized
    public static native @ByVal llama_batch llama_batch_init(
                int n_tokens,
                int embd,
                int n_seq_max);

    // Frees a batch of tokens allocated with llama_batch_init()
    public static native void llama_batch_free(@ByVal llama_batch batch);

    // Processes a batch of tokens with the ecoder part of the encoder-decoder model.
    // Stores the encoder output internally for later use by the decoder cross-attention layers.
    //   0 - success
    // < 0 - error. the KV cache state is restored to the state before this call
    public static native int llama_encode(
                llama_context ctx,
                  @ByVal llama_batch batch);

    // Positive return values does not mean a fatal error, but rather a warning.
    //   0 - success
    //   1 - could not find a KV slot for the batch (try reducing the size of the batch or increase the context)
    // < 0 - error. the KV cache state is restored to the state before this call
    public static native int llama_decode(
                llama_context ctx,
                  @ByVal llama_batch batch);

    // Set the number of threads used for decoding
    // n_threads is the number of threads used for generation (single token)
    // n_threads_batch is the number of threads used for prompt and batch processing (multiple tokens)
    public static native void llama_set_n_threads(llama_context ctx, int n_threads, int n_threads_batch);

    // Get the number of threads used for generation of a single token.
    public static native int llama_n_threads(llama_context ctx);

    // Get the number of threads used for prompt and batch processing (multiple token).
    public static native int llama_n_threads_batch(llama_context ctx);

    // Set whether the model is in embeddings mode or not
    // If true, embeddings will be returned but logits will not
    public static native void llama_set_embeddings(llama_context ctx, @Cast("bool") boolean embeddings);

    // Set whether to use causal attention or not
    // If set to true, the model will only attend to the past tokens
    public static native void llama_set_causal_attn(llama_context ctx, @Cast("bool") boolean causal_attn);

    // Set abort callback
    

    // Wait until all computations are finished
    // This is automatically done when using one of the functions below to obtain the computation results
    // and is not necessary to call it explicitly in most cases
    public static native void llama_synchronize(llama_context ctx);

    // Token logits obtained from the last call to llama_decode()
    // The logits for which llama_batch.logits[i] != 0 are stored contiguously
    // in the order they have appeared in the batch.
    // Rows: number of tokens for which llama_batch.logits[i] != 0
    // Cols: n_vocab
    public static native FloatPointer llama_get_logits(llama_context ctx);

    // Logits for the ith token. For positive indices, Equivalent to:
    // llama_get_logits(ctx) + ctx->output_ids[i]*n_vocab
    // Negative indicies can be used to access logits in reverse order, -1 is the last logit.
    // returns NULL for invalid ids.
    public static native FloatPointer llama_get_logits_ith(llama_context ctx, int i);

    // Get all output token embeddings.
    // when pooling_type == LLAMA_POOLING_TYPE_NONE or when using a generative model,
    // the embeddings for which llama_batch.logits[i] != 0 are stored contiguously
    // in the order they have appeared in the batch.
    // shape: [n_outputs*n_embd]
    // Otherwise, returns NULL.
    public static native FloatPointer llama_get_embeddings(llama_context ctx);

    // Get the embeddings for the ith token. For positive indices, Equivalent to:
    // llama_get_embeddings(ctx) + ctx->output_ids[i]*n_embd
    // Negative indicies can be used to access embeddings in reverse order, -1 is the last embedding.
    // shape: [n_embd] (1-dimensional)
    // returns NULL for invalid ids.
    public static native FloatPointer llama_get_embeddings_ith(llama_context ctx, int i);

    // Get the embeddings for a sequence id
    // Returns NULL if pooling_type is LLAMA_POOLING_TYPE_NONE
    // when pooling_type == LLAMA_POOLING_TYPE_RANK, returns float[1] with the rank of the sequence
    // otherwise: float[n_embd] (1-dimensional)
    public static native FloatPointer llama_get_embeddings_seq(llama_context ctx, @Cast("llama_seq_id") int seq_id);

    //
    // Vocab
    //

    public static native @Cast("const char*") BytePointer llama_vocab_get_text(@Const llama_vocab vocab, @Cast("llama_token") int token);

    public static native float llama_vocab_get_score(@Const llama_vocab vocab, @Cast("llama_token") int token);

    public static native llama_token_attr llama_vocab_get_attr(@Const llama_vocab vocab, @Cast("llama_token") int token);

    // Check if the token is supposed to end generation (end-of-generation, eg. EOS, EOT, etc.)
    public static native @Cast("bool") boolean llama_vocab_is_eog(@Const llama_vocab vocab, @Cast("llama_token") int token);

    // Identify if Token Id is a control token or a render-able token
    public static native @Cast("bool") boolean llama_vocab_is_control(@Const llama_vocab vocab, @Cast("llama_token") int token);

    // Special tokens
    public static native @Cast("llama_token") int llama_vocab_bos(@Const llama_vocab vocab); // beginning-of-sentence
    public static native @Cast("llama_token") int llama_vocab_eos(@Const llama_vocab vocab); // end-of-sentence
    public static native @Cast("llama_token") int llama_vocab_eot(@Const llama_vocab vocab); // end-of-turn
    public static native @Cast("llama_token") int llama_vocab_sep(@Const llama_vocab vocab); // sentence separator
    public static native @Cast("llama_token") int llama_vocab_nl(@Const llama_vocab vocab); // next-line
    public static native @Cast("llama_token") int llama_vocab_pad(@Const llama_vocab vocab); // padding

    public static native @Cast("bool") boolean llama_vocab_get_add_bos(@Const llama_vocab vocab);
    public static native @Cast("bool") boolean llama_vocab_get_add_eos(@Const llama_vocab vocab);

    public static native @Cast("llama_token") int llama_vocab_fim_pre(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_vocab_fim_suf(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_vocab_fim_mid(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_vocab_fim_pad(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_vocab_fim_rep(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_vocab_fim_sep(@Const llama_vocab vocab);

    public static native @Cast("const char*") BytePointer llama_token_get_text(@Const llama_vocab vocab, @Cast("llama_token") int token);
    public static native float llama_token_get_score(@Const llama_vocab vocab, @Cast("llama_token") int token);
    public static native llama_token_attr llama_token_get_attr(@Const llama_vocab vocab, @Cast("llama_token") int token);
    public static native @Cast("bool") boolean llama_token_is_eog(@Const llama_vocab vocab, @Cast("llama_token") int token);
    public static native @Cast("bool") boolean llama_token_is_control(@Const llama_vocab vocab, @Cast("llama_token") int token);
    public static native @Cast("llama_token") int llama_token_bos(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_eos(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_eot(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_cls(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_sep(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_nl(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_pad(@Const llama_vocab vocab);
    public static native @Cast("bool") boolean llama_add_bos_token(@Const llama_vocab vocab);
    public static native @Cast("bool") boolean llama_add_eos_token(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_fim_pre(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_fim_suf(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_fim_mid(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_fim_pad(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_fim_rep(@Const llama_vocab vocab);
    public static native @Cast("llama_token") int llama_token_fim_sep(@Const llama_vocab vocab);

    // CLS is equivalent to BOS
    public static native @Cast("llama_token") int llama_vocab_cls(@Const llama_vocab vocab);

    //
    // Tokenization
    //
    // The API is thread-safe.
    //

    /** \details Convert the provided text into tokens.
     *  @param tokens The tokens pointer must be large enough to hold the resulting tokens.
     *  @return Returns the number of tokens on success, no more than n_tokens_max
     *  @return Returns a negative number on failure - the number of tokens that would have been returned
     *  @param add_special Allow to add BOS and EOS tokens if model is configured to do so.
     *  @param parse_special Allow tokenizing special and/or control tokens which otherwise are not exposed and treated
     *                       as plaintext. Does not insert a leading space. */
    public static native int llama_tokenize(
            @Const llama_vocab vocab,
                          @Cast("const char*") BytePointer text,
                             int text_len,
                         @Cast("llama_token*") IntPointer tokens,
                             int n_tokens_max,
                                @Cast("bool") boolean add_special,
                                @Cast("bool") boolean parse_special);
    public static native int llama_tokenize(
            @Const llama_vocab vocab,
                          String text,
                             int text_len,
                         @Cast("llama_token*") IntBuffer tokens,
                             int n_tokens_max,
                                @Cast("bool") boolean add_special,
                                @Cast("bool") boolean parse_special);
    public static native int llama_tokenize(
            @Const llama_vocab vocab,
                          @Cast("const char*") BytePointer text,
                             int text_len,
                         @Cast("llama_token*") int[] tokens,
                             int n_tokens_max,
                                @Cast("bool") boolean add_special,
                                @Cast("bool") boolean parse_special);
    public static native int llama_tokenize(
            @Const llama_vocab vocab,
                          String text,
                             int text_len,
                         @Cast("llama_token*") IntPointer tokens,
                             int n_tokens_max,
                                @Cast("bool") boolean add_special,
                                @Cast("bool") boolean parse_special);
    public static native int llama_tokenize(
            @Const llama_vocab vocab,
                          @Cast("const char*") BytePointer text,
                             int text_len,
                         @Cast("llama_token*") IntBuffer tokens,
                             int n_tokens_max,
                                @Cast("bool") boolean add_special,
                                @Cast("bool") boolean parse_special);
    public static native int llama_tokenize(
            @Const llama_vocab vocab,
                          String text,
                             int text_len,
                         @Cast("llama_token*") int[] tokens,
                             int n_tokens_max,
                                @Cast("bool") boolean add_special,
                                @Cast("bool") boolean parse_special);

    // Token Id -> Piece.
    // Uses the vocabulary in the provided context.
    // Does not write null terminator to the buffer.
    // User can skip up to 'lstrip' leading spaces before copying (useful when encoding/decoding multiple tokens with 'add_space_prefix')
    // @param special If true, special tokens are rendered in the output.
    public static native int llama_token_to_piece(
                  @Const llama_vocab vocab,
                               @Cast("llama_token") int token,
                                      @Cast("char*") BytePointer buf,
                                   int length,
                                   int lstrip,
                                      @Cast("bool") boolean special);
    public static native int llama_token_to_piece(
                  @Const llama_vocab vocab,
                               @Cast("llama_token") int token,
                                      @Cast("char*") ByteBuffer buf,
                                   int length,
                                   int lstrip,
                                      @Cast("bool") boolean special);
    public static native int llama_token_to_piece(
                  @Const llama_vocab vocab,
                               @Cast("llama_token") int token,
                                      @Cast("char*") byte[] buf,
                                   int length,
                                   int lstrip,
                                      @Cast("bool") boolean special);

    /** \details Convert the provided tokens into text (inverse of llama_tokenize()).
     *  @param text The char pointer must be large enough to hold the resulting text.
     *  @return Returns the number of chars/bytes on success, no more than text_len_max.
     *  @return Returns a negative number on failure - the number of chars/bytes that would have been returned.
     *  @param remove_special Allow to remove BOS and EOS tokens if model is configured to do so.
     *  @param unparse_special If true, special tokens are rendered in the output. */
    public static native int llama_detokenize(
            @Const llama_vocab vocab,
                   @Cast("const llama_token*") IntPointer tokens,
                             int n_tokens,
                                @Cast("char*") BytePointer text,
                             int text_len_max,
                                @Cast("bool") boolean remove_special,
                                @Cast("bool") boolean unparse_special);
    public static native int llama_detokenize(
            @Const llama_vocab vocab,
                   @Cast("const llama_token*") IntBuffer tokens,
                             int n_tokens,
                                @Cast("char*") ByteBuffer text,
                             int text_len_max,
                                @Cast("bool") boolean remove_special,
                                @Cast("bool") boolean unparse_special);
    public static native int llama_detokenize(
            @Const llama_vocab vocab,
                   @Cast("const llama_token*") int[] tokens,
                             int n_tokens,
                                @Cast("char*") byte[] text,
                             int text_len_max,
                                @Cast("bool") boolean remove_special,
                                @Cast("bool") boolean unparse_special);

    //
    // Chat templates
    //

    /** Apply chat template. Inspired by hf apply_chat_template() on python.
     *  Both "model" and "custom_template" are optional, but at least one is required. "custom_template" has higher precedence than "model"
     *  NOTE: This function does not use a jinja parser. It only support a pre-defined list of template. See more: https://github.com/ggml-org/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template
     *  @param tmpl A Jinja template to use for this chat. If this is nullptr, the model’s default chat template will be used instead.
     *  @param chat Pointer to a list of multiple llama_chat_message
     *  @param n_msg Number of llama_chat_message in this chat
     *  @param add_ass Whether to end the prompt with the token(s) that indicate the start of an assistant message.
     *  @param buf A buffer to hold the output formatted prompt. The recommended alloc size is 2 * (total number of characters of all messages)
     *  @param length The size of the allocated buffer
     *  @return The total number of bytes of the formatted prompt. If is it larger than the size of buffer, you may need to re-alloc it and then re-apply the template. */
    public static native int llama_chat_apply_template(
                                @Cast("const char*") BytePointer tmpl,
           @Const llama_chat_message chat,
                                    @Cast("size_t") long n_msg,
                                      @Cast("bool") boolean add_ass,
                                      @Cast("char*") BytePointer buf,
                                   int length);
    public static native int llama_chat_apply_template(
                                String tmpl,
           @Const llama_chat_message chat,
                                    @Cast("size_t") long n_msg,
                                      @Cast("bool") boolean add_ass,
                                      @Cast("char*") ByteBuffer buf,
                                   int length);
    public static native int llama_chat_apply_template(
                                @Cast("const char*") BytePointer tmpl,
           @Const llama_chat_message chat,
                                    @Cast("size_t") long n_msg,
                                      @Cast("bool") boolean add_ass,
                                      @Cast("char*") byte[] buf,
                                   int length);
    public static native int llama_chat_apply_template(
                                String tmpl,
           @Const llama_chat_message chat,
                                    @Cast("size_t") long n_msg,
                                      @Cast("bool") boolean add_ass,
                                      @Cast("char*") BytePointer buf,
                                   int length);
    public static native int llama_chat_apply_template(
                                @Cast("const char*") BytePointer tmpl,
           @Const llama_chat_message chat,
                                    @Cast("size_t") long n_msg,
                                      @Cast("bool") boolean add_ass,
                                      @Cast("char*") ByteBuffer buf,
                                   int length);
    public static native int llama_chat_apply_template(
                                String tmpl,
           @Const llama_chat_message chat,
                                    @Cast("size_t") long n_msg,
                                      @Cast("bool") boolean add_ass,
                                      @Cast("char*") byte[] buf,
                                   int length);

    // Get list of built-in chat templates
    public static native int llama_chat_builtin_templates(@Cast("const char**") PointerPointer output, @Cast("size_t") long len);
    public static native int llama_chat_builtin_templates(@Cast("const char**") @ByPtrPtr BytePointer output, @Cast("size_t") long len);
    public static native int llama_chat_builtin_templates(@Cast("const char**") @ByPtrPtr ByteBuffer output, @Cast("size_t") long len);
    public static native int llama_chat_builtin_templates(@Cast("const char**") @ByPtrPtr byte[] output, @Cast("size_t") long len);
// Targeting ../llama_sampler_context_t.java


// Targeting ../llama_sampler_i.java


// Targeting ../llama_sampler.java



    // mirror of llama_sampler_i:
    public static native llama_sampler llama_sampler_init(@Const llama_sampler_i iface, llama_sampler_context_t ctx);
    public static native @Cast("const char*") BytePointer llama_sampler_name(@Const llama_sampler smpl);
    public static native void llama_sampler_accept(      llama_sampler smpl, @Cast("llama_token") int token);
    public static native void llama_sampler_apply(      llama_sampler smpl, llama_token_data_array cur_p);
    public static native void llama_sampler_reset(      llama_sampler smpl);
    public static native llama_sampler llama_sampler_clone(@Const llama_sampler smpl);
    // important: do not free if the sampler has been added to a llama_sampler_chain (via llama_sampler_chain_add)
    public static native void llama_sampler_free(      llama_sampler smpl);

    // llama_sampler_chain
    // a type of llama_sampler that can chain multiple samplers one after another

    public static native llama_sampler llama_sampler_chain_init(@ByVal llama_sampler_chain_params params);

    // important: takes ownership of the sampler object and will free it when llama_sampler_free is called
    public static native void llama_sampler_chain_add(      llama_sampler chain, llama_sampler smpl);
    public static native llama_sampler llama_sampler_chain_get(@Const llama_sampler chain, int i);
    public static native int llama_sampler_chain_n(@Const llama_sampler chain);

    // after removing a sampler, the chain will no longer own it, and it will not be freed when the chain is freed
    public static native llama_sampler llama_sampler_chain_remove(   llama_sampler chain, int i);

    // available samplers:

    public static native llama_sampler llama_sampler_init_greedy();
    public static native llama_sampler llama_sampler_init_dist(@Cast("uint32_t") int seed);

    /** \details Sorts candidate tokens by their logits in descending order and calculate probabilities based on logits.
     *  NOTE: Avoid using on the full vocabulary as the sorting can become slow. For example, apply top-k or top-p sampling first. */
    public static native llama_sampler llama_sampler_init_softmax();

    /** \details Top-K sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751 */
    public static native llama_sampler llama_sampler_init_top_k(int k);

    /** \details Nucleus sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751 */
    public static native llama_sampler llama_sampler_init_top_p(float p, @Cast("size_t") long min_keep);

    /** \details Minimum P sampling as described in https://github.com/ggml-org/llama.cpp/pull/3841 */
    public static native llama_sampler llama_sampler_init_min_p(float p, @Cast("size_t") long min_keep);

    /** \details Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666. */
    public static native llama_sampler llama_sampler_init_typical(float p, @Cast("size_t") long min_keep);

    /** #details Updates the logits l_i{@code  = l_i/t. When t <= 0.0f, the maximum logit is kept at it's original value, the rest are set to -inf */
    public static native llama_sampler llama_sampler_init_temp(float t);

    /** \details Dynamic temperature implementation (a.k.a. entropy) described in the paper https://arxiv.org/abs/2309.02772. */
    public static native llama_sampler llama_sampler_init_temp_ext(float t, float delta, float exponent);

    /** \details XTC sampler as described in https://github.com/oobabooga/text-generation-webui/pull/6335 */
    public static native llama_sampler llama_sampler_init_xtc(float p, float t,     @Cast("size_t") long min_keep, @Cast("uint32_t") int seed);

    /** \details Top n sigma sampling as described in academic paper "Top-nσ: Not All Logits Are You Need" https://arxiv.org/pdf/2411.07641 */
    public static native llama_sampler llama_sampler_init_top_n_sigma(float n);

    /** \details Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
     *  @param candidates A vector of {@code llama_token_data} containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
     *  @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
     *  @param eta The learning rate used to update {@code mu} based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause {@code mu} to be updated more quickly, while a smaller learning rate will result in slower updates.
     *  @param m The number of tokens considered in the estimation of {@code s_hat}. This is an arbitrary value that is used to calculate {@code s_hat}, which in turn helps to calculate the value of {@code k}. In the paper, they use {@code m = 100}, but you can experiment with different values to see how it affects the performance of the algorithm.
     *  @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy ({@code 2 * tau}) and is updated in the algorithm based on the error between the target and observed surprisal. */
    public static native llama_sampler llama_sampler_init_mirostat(
                                 int n_vocab,
                                @Cast("uint32_t") int seed,
                                   float tau,
                                   float eta,
                                 int m);

    /** \details Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
     *  @param candidates A vector of {@code llama_token_data} containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
     *  @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
     *  @param eta The learning rate used to update {@code mu} based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause {@code mu} to be updated more quickly, while a smaller learning rate will result in slower updates.
     *  @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy ({@code 2 * tau}) and is updated in the algorithm based on the error between the target and observed surprisal. */
    public static native llama_sampler llama_sampler_init_mirostat_v2(
                                @Cast("uint32_t") int seed,
                                   float tau,
                                   float eta);

    public static native llama_sampler llama_sampler_init_grammar(
                @Const llama_vocab vocab,
                              @Cast("const char*") BytePointer grammar_str,
                              @Cast("const char*") BytePointer grammar_root);
    public static native llama_sampler llama_sampler_init_grammar(
                @Const llama_vocab vocab,
                              String grammar_str,
                              String grammar_root);

    public static native llama_sampler llama_sampler_init_grammar_lazy(
                @Const llama_vocab vocab,
                              @Cast("const char*") BytePointer grammar_str,
                              @Cast("const char*") BytePointer grammar_root,
                             @Cast("const char**") PointerPointer trigger_words,
                                    @Cast("size_t") long num_trigger_words,
                       @Cast("const llama_token*") IntPointer trigger_tokens,
                                    @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy(
                @Const llama_vocab vocab,
                              @Cast("const char*") BytePointer grammar_str,
                              @Cast("const char*") BytePointer grammar_root,
                             @Cast("const char**") @ByPtrPtr BytePointer trigger_words,
                                    @Cast("size_t") long num_trigger_words,
                       @Cast("const llama_token*") IntPointer trigger_tokens,
                                    @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy(
                @Const llama_vocab vocab,
                              String grammar_str,
                              String grammar_root,
                             @Cast("const char**") @ByPtrPtr ByteBuffer trigger_words,
                                    @Cast("size_t") long num_trigger_words,
                       @Cast("const llama_token*") IntBuffer trigger_tokens,
                                    @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy(
                @Const llama_vocab vocab,
                              @Cast("const char*") BytePointer grammar_str,
                              @Cast("const char*") BytePointer grammar_root,
                             @Cast("const char**") @ByPtrPtr byte[] trigger_words,
                                    @Cast("size_t") long num_trigger_words,
                       @Cast("const llama_token*") int[] trigger_tokens,
                                    @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy(
                @Const llama_vocab vocab,
                              String grammar_str,
                              String grammar_root,
                             @Cast("const char**") @ByPtrPtr BytePointer trigger_words,
                                    @Cast("size_t") long num_trigger_words,
                       @Cast("const llama_token*") IntPointer trigger_tokens,
                                    @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy(
                @Const llama_vocab vocab,
                              @Cast("const char*") BytePointer grammar_str,
                              @Cast("const char*") BytePointer grammar_root,
                             @Cast("const char**") @ByPtrPtr ByteBuffer trigger_words,
                                    @Cast("size_t") long num_trigger_words,
                       @Cast("const llama_token*") IntBuffer trigger_tokens,
                                    @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy(
                @Const llama_vocab vocab,
                              String grammar_str,
                              String grammar_root,
                             @Cast("const char**") @ByPtrPtr byte[] trigger_words,
                                    @Cast("size_t") long num_trigger_words,
                       @Cast("const llama_token*") int[] trigger_tokens,
                                    @Cast("size_t") long num_trigger_tokens);


    /** \details Lazy grammar sampler, introduced in https://github.com/ggml-org/llama.cpp/pull/9639
     *  @param trigger_patterns A list of patterns that will trigger the grammar sampler. Pattern will be matched from the start of the generation output, and grammar sampler will be fed content starting from its first match group.
     *  @param trigger_tokens A list of tokens that will trigger the grammar sampler. Grammar sampler will be fed content starting from the trigger token included. */
    public static native llama_sampler llama_sampler_init_grammar_lazy_patterns(
            @Const llama_vocab vocab,
                          @Cast("const char*") BytePointer grammar_str,
                          @Cast("const char*") BytePointer grammar_root,
                         @Cast("const char**") PointerPointer trigger_patterns,
                                @Cast("size_t") long num_trigger_patterns,
                   @Cast("const llama_token*") IntPointer trigger_tokens,
                                @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy_patterns(
            @Const llama_vocab vocab,
                          @Cast("const char*") BytePointer grammar_str,
                          @Cast("const char*") BytePointer grammar_root,
                         @Cast("const char**") @ByPtrPtr BytePointer trigger_patterns,
                                @Cast("size_t") long num_trigger_patterns,
                   @Cast("const llama_token*") IntPointer trigger_tokens,
                                @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy_patterns(
            @Const llama_vocab vocab,
                          String grammar_str,
                          String grammar_root,
                         @Cast("const char**") @ByPtrPtr ByteBuffer trigger_patterns,
                                @Cast("size_t") long num_trigger_patterns,
                   @Cast("const llama_token*") IntBuffer trigger_tokens,
                                @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy_patterns(
            @Const llama_vocab vocab,
                          @Cast("const char*") BytePointer grammar_str,
                          @Cast("const char*") BytePointer grammar_root,
                         @Cast("const char**") @ByPtrPtr byte[] trigger_patterns,
                                @Cast("size_t") long num_trigger_patterns,
                   @Cast("const llama_token*") int[] trigger_tokens,
                                @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy_patterns(
            @Const llama_vocab vocab,
                          String grammar_str,
                          String grammar_root,
                         @Cast("const char**") @ByPtrPtr BytePointer trigger_patterns,
                                @Cast("size_t") long num_trigger_patterns,
                   @Cast("const llama_token*") IntPointer trigger_tokens,
                                @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy_patterns(
            @Const llama_vocab vocab,
                          @Cast("const char*") BytePointer grammar_str,
                          @Cast("const char*") BytePointer grammar_root,
                         @Cast("const char**") @ByPtrPtr ByteBuffer trigger_patterns,
                                @Cast("size_t") long num_trigger_patterns,
                   @Cast("const llama_token*") IntBuffer trigger_tokens,
                                @Cast("size_t") long num_trigger_tokens);
    public static native llama_sampler llama_sampler_init_grammar_lazy_patterns(
            @Const llama_vocab vocab,
                          String grammar_str,
                          String grammar_root,
                         @Cast("const char**") @ByPtrPtr byte[] trigger_patterns,
                                @Cast("size_t") long num_trigger_patterns,
                   @Cast("const llama_token*") int[] trigger_tokens,
                                @Cast("size_t") long num_trigger_tokens);


    /** NOTE: Avoid using on the full vocabulary as searching for repeated tokens can become slow. For example, apply top-k or top-p sampling first. */
    public static native llama_sampler llama_sampler_init_penalties(
                                 int penalty_last_n,
                                   float penalty_repeat,
                                   float penalty_freq,
                                   float penalty_present); // 0.0 = disabled

    /**  \details DRY sampler, designed by p-e-w, as described in: https://github.com/oobabooga/text-generation-webui/pull/5677, porting Koboldcpp implementation authored by pi6am: https://github.com/LostRuins/koboldcpp/pull/982 */
    public static native llama_sampler llama_sampler_init_dry(
                @Const llama_vocab vocab,
                                 int n_ctx_train,
                                   float dry_multiplier,
                                   float dry_base,
                                 int dry_allowed_length,
                                 int dry_penalty_last_n,
                              @Cast("const char**") PointerPointer seq_breakers,
                                  @Cast("size_t") long num_breakers);
    public static native llama_sampler llama_sampler_init_dry(
                @Const llama_vocab vocab,
                                 int n_ctx_train,
                                   float dry_multiplier,
                                   float dry_base,
                                 int dry_allowed_length,
                                 int dry_penalty_last_n,
                              @Cast("const char**") @ByPtrPtr BytePointer seq_breakers,
                                  @Cast("size_t") long num_breakers);
    public static native llama_sampler llama_sampler_init_dry(
                @Const llama_vocab vocab,
                                 int n_ctx_train,
                                   float dry_multiplier,
                                   float dry_base,
                                 int dry_allowed_length,
                                 int dry_penalty_last_n,
                              @Cast("const char**") @ByPtrPtr ByteBuffer seq_breakers,
                                  @Cast("size_t") long num_breakers);
    public static native llama_sampler llama_sampler_init_dry(
                @Const llama_vocab vocab,
                                 int n_ctx_train,
                                   float dry_multiplier,
                                   float dry_base,
                                 int dry_allowed_length,
                                 int dry_penalty_last_n,
                              @Cast("const char**") @ByPtrPtr byte[] seq_breakers,
                                  @Cast("size_t") long num_breakers);

    public static native llama_sampler llama_sampler_init_logit_bias(
                                 int n_vocab,
                                 int n_logit_bias,
                  @Const llama_logit_bias logit_bias);

    // this sampler is meant to be used for fill-in-the-middle infilling
    // it's supposed to be used after top_k + top_p sampling
    //
    // 1. if the sum of the EOG probs times the number of candidates is higher than the sum of the other probs -> pick EOG
    // 2. combine probs of tokens that have the same prefix
    //
    // example:
    //
    // - before:
    //   "hel":   0.5
    //   "hell":  0.2
    //   "hello": 0.1
    //   "dummy": 0.1
    //
    // - after:
    //   "hel":   0.8
    //   "dummy": 0.1
    //
    // 3. discard non-EOG tokens with low prob
    // 4. if no tokens are left -> pick EOT
    //
    public static native llama_sampler llama_sampler_init_infill(@Const llama_vocab vocab);

    // Returns the seed used by the sampler if applicable, LLAMA_DEFAULT_SEED otherwise
    public static native @Cast("uint32_t") int llama_sampler_get_seed(@Const llama_sampler smpl);

    /** \details Sample and accept a token from the idx-th output of the last evaluation */
    //
    // Shorthand for:
    //    const auto * logits = llama_get_logits_ith(ctx, idx);
    //    llama_token_data_array cur_p = { ... init from logits ... };
    //    llama_sampler_apply(smpl, &cur_p);
    //    auto token = cur_p.data[cur_p.selected].id;
    //    llama_sampler_accept(smpl, token);
    //    return token;
    // Returns the sampled token
    public static native @Cast("llama_token") int llama_sampler_sample(llama_sampler smpl, llama_context ctx, int idx);

    // TODO: extend in the future
    //LLAMA_API void llama_decode_with_sampler(struct llama_context * ctx, struct llama_sampler * smpl, struct llama_batch batch, ...);

    //
    // Model split
    //

    /** \details Build a split GGUF final path for this chunk.
     *           llama_split_path(split_path, sizeof(split_path), "/models/ggml-model-q4_0", 2, 4) => split_path = "/models/ggml-model-q4_0-00002-of-00004.gguf" */
    //  Returns the split_path length.
    public static native int llama_split_path(@Cast("char*") BytePointer split_path, @Cast("size_t") long maxlen, @Cast("const char*") BytePointer path_prefix, int split_no, int split_count);
    public static native int llama_split_path(@Cast("char*") ByteBuffer split_path, @Cast("size_t") long maxlen, String path_prefix, int split_no, int split_count);
    public static native int llama_split_path(@Cast("char*") byte[] split_path, @Cast("size_t") long maxlen, @Cast("const char*") BytePointer path_prefix, int split_no, int split_count);
    public static native int llama_split_path(@Cast("char*") BytePointer split_path, @Cast("size_t") long maxlen, String path_prefix, int split_no, int split_count);
    public static native int llama_split_path(@Cast("char*") ByteBuffer split_path, @Cast("size_t") long maxlen, @Cast("const char*") BytePointer path_prefix, int split_no, int split_count);
    public static native int llama_split_path(@Cast("char*") byte[] split_path, @Cast("size_t") long maxlen, String path_prefix, int split_no, int split_count);

    /** \details Extract the path prefix from the split_path if and only if the split_no and split_count match.
     *           llama_split_prefix(split_prefix, 64, "/models/ggml-model-q4_0-00002-of-00004.gguf", 2, 4) => split_prefix = "/models/ggml-model-q4_0" */
    //  Returns the split_prefix length.
    public static native int llama_split_prefix(@Cast("char*") BytePointer split_prefix, @Cast("size_t") long maxlen, @Cast("const char*") BytePointer split_path, int split_no, int split_count);
    public static native int llama_split_prefix(@Cast("char*") ByteBuffer split_prefix, @Cast("size_t") long maxlen, String split_path, int split_no, int split_count);
    public static native int llama_split_prefix(@Cast("char*") byte[] split_prefix, @Cast("size_t") long maxlen, @Cast("const char*") BytePointer split_path, int split_no, int split_count);
    public static native int llama_split_prefix(@Cast("char*") BytePointer split_prefix, @Cast("size_t") long maxlen, String split_path, int split_no, int split_count);
    public static native int llama_split_prefix(@Cast("char*") ByteBuffer split_prefix, @Cast("size_t") long maxlen, @Cast("const char*") BytePointer split_path, int split_no, int split_count);
    public static native int llama_split_prefix(@Cast("char*") byte[] split_prefix, @Cast("size_t") long maxlen, String split_path, int split_no, int split_count);

    // Print system information
    public static native @Cast("const char*") BytePointer llama_print_system_info();

    // Set callback for all future logging events.
    // If this is not called, or NULL is supplied, everything is output on stderr.
    public static native void llama_log_set(ggml_log_callback log_callback, Pointer user_data);
// Targeting ../llama_perf_context_data.java


// Targeting ../llama_perf_sampler_data.java



    public static native @ByVal llama_perf_context_data llama_perf_context(@Const llama_context ctx);
    public static native void llama_perf_context_print(@Const llama_context ctx);
    public static native void llama_perf_context_reset(      llama_context ctx);

    // NOTE: the following work only with samplers constructed via llama_sampler_chain_init
    public static native @ByVal llama_perf_sampler_data llama_perf_sampler(@Const llama_sampler chain);
    public static native void llama_perf_sampler_print(@Const llama_sampler chain);
    public static native void llama_perf_sampler_reset(      llama_sampler chain);

// #ifdef __cplusplus
// #endif

// #endif // LLAMA_H


}
